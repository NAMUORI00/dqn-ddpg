# 동일 환경 DQN vs DDPG 비교 실험

## 개요

이 기능은 DQN과 DDPG 알고리즘을 **동일한 환경**에서 실험하여 공정한 비교 분석을 제공합니다. 기존의 서로 다른 환경(CartPole vs Pendulum) 대신, 새로운 ContinuousCartPole 환경에서 두 알고리즘을 모두 테스트할 수 있습니다.

## 핵심 아이디어

### 문제점
- 기존: DQN은 CartPole-v1(이산), DDPG는 Pendulum-v1(연속)에서 실험
- 환경 차이로 인해 순수한 알고리즘 성능 비교가 어려움

### 해결책
- **ContinuousCartPole**: CartPole-v1과 동일한 물리학, 연속 행동 공간
- **DiscretizedDQN**: 연속 행동 공간에서 작동하는 DQN 변형
- 동일 환경에서 두 알고리즘의 순수 성능 비교 가능

## 구현 구조

### 1. ContinuousCartPole 환경
```python
# 위치: src/environments/continuous_cartpole.py
- CartPole-v1과 동일한 물리 상수 및 계산
- 행동 공간: Box([-1, 1], shape=(1,)) 
- 연속 행동을 force로 직접 매핑
```

### 2. DiscretizedDQNAgent
```python
# 위치: src/agents/discretized_dqn_agent.py
- 기존 DQN 에이전트 상속
- 연속 행동 공간을 N개 구간으로 이산화
- 이산 인덱스 ↔ 연속 값 매핑 테이블
- 결정적 정책 분석 메서드 제공
```

### 3. 비교 실험 프레임워크
```python
# 위치: experiments/same_environment_comparison.py
- 동일 조건에서 두 에이전트 훈련
- 학습 성능 비교
- 결정적 정책 특성 분석
- 행동 선택 패턴 비교
```

## 사용 방법

### 빠른 테스트
```bash
# 기본 동작 확인
python tests/test_same_environment.py

# 짧은 훈련 데모
python tests/quick_comparison_demo.py
```

### 전체 비교 실험
```bash
# 완전한 비교 실험 (시간 소요)
python experiments/same_environment_comparison.py
```

### 프로그래밍 방식 사용
```python
from src.environments.continuous_cartpole import create_continuous_cartpole_env
from src.agents.discretized_dqn_agent import DiscretizedDQNAgent
from src.agents.ddpg_agent import DDPGAgent

# 환경 생성
env = create_continuous_cartpole_env()

# 에이전트 생성
dqn_agent = DiscretizedDQNAgent(
    state_dim=4,
    action_bound=1.0,
    num_actions=21  # 이산화 해상도
)

ddpg_agent = DDPGAgent(
    state_dim=4,
    action_dim=1,
    action_bound=1.0
)

# 동일 상태에서 행동 비교
state, _ = env.reset()
dqn_action = dqn_agent.get_deterministic_action(state)
ddpg_action = ddpg_agent.get_deterministic_action(state)
```

## 주요 분석 메트릭

### 1. 학습 성능
- 에피소드별 점수 변화
- 수렴 속도 및 안정성
- 최종 평가 성능

### 2. 결정적 정책 특성
- **DQN**: Q-값 기반 암묵적 결정성
- **DDPG**: 액터 네트워크 기반 명시적 결정성
- 일관성 점수 (동일 상태에서 동일 행동 선택률)

### 3. 행동 분석
- 행동 선택 분포
- 두 알고리즘 간 행동 차이
- 이산화 오차 분석 (DQN)

## 교육적 가치

### 1. 알고리즘 이해 심화
- 결정적 정책의 서로 다른 구현 방식 비교
- 이산 vs 연속 행동 공간 처리 방법
- 탐험 전략의 차이점

### 2. 공정한 성능 비교
- 환경 변수 제거로 순수 알고리즘 성능 평가
- 동일 조건에서의 학습 특성 분석
- 실제 적용 시 알고리즘 선택 기준 제공

### 3. 구현 설계 학습
- 기존 알고리즘의 환경 적응 방법
- 연속-이산 변환 기법
- 호환성 유지하면서 기능 확장하는 방법

## 결과 해석 가이드

### DQN (DiscretizedDQN)
- **장점**: 안정적 학습, 명확한 행동 선택
- **단점**: 이산화 오차, 제한된 행동 해상도
- **적합한 경우**: 명확한 선택이 필요한 문제

### DDPG
- **장점**: 연속 정밀 제어, 무한 행동 해상도
- **단점**: 학습 불안정성, 하이퍼파라미터 민감성
- **적합한 경우**: 세밀한 연속 제어가 필요한 문제

### 결정성 비교
- 두 알고리즘 모두 높은 결정성 달성 가능
- DQN: Q-값 계산 후 argmax (간접적)
- DDPG: 액터 직접 출력 (직접적)

## 확장 가능성

### 1. 다른 환경으로 확장
```python
# 다른 연속 제어 환경에서도 동일한 방식 적용 가능
env = gym.make("MountainCarContinuous-v0")
# DiscretizedDQN과 DDPG 비교
```

### 2. 하이브리드 접근법
```python
# 이산-연속 혼합 행동 공간 처리
# 다차원 연속 행동 공간 확장
```

### 3. 다른 알고리즘 추가
```python
# SAC, TD3 등 다른 연속 제어 알고리즘
# A3C, PPO 등 정책 기반 알고리즘
```

## 기술적 세부사항

### ContinuousCartPole 물리
```python
# CartPole-v1과 동일한 물리 상수
gravity = 9.8
masscart = 1.0  
masspole = 0.1
force_mag = 10.0
tau = 0.02  # 시간 스텝

# 연속 행동 매핑
force = action[0] * force_mag  # [-1,1] → [-10,10]
```

### 이산화 메커니즘
```python
# N개 구간으로 균등 분할
action_values = np.linspace(-1.0, 1.0, num_actions)

# 연속 → 이산
discrete_idx = np.argmin(|action_values - continuous_action|)

# 이산 → 연속  
continuous_action = action_values[discrete_idx]
```

## 관련 파일

### 핵심 구현
- `src/environments/continuous_cartpole.py` - 새로운 환경
- `src/agents/discretized_dqn_agent.py` - 이산화 DQN
- `experiments/same_environment_comparison.py` - 비교 실험

### 테스트 및 데모
- `tests/test_same_environment.py` - 기본 동작 테스트
- `tests/quick_comparison_demo.py` - 빠른 데모

### 문서
- `docs/documentation/동일환경_비교_시스템_가이드.md` - 이 문서
- `docs/documentation/개발_진행_로그.md` - 개발 과정 및 비디오 기능 가이드