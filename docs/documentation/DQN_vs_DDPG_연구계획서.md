# DQN vs DDPG 결정적 정책 비교분석 연구계획서

## 📋 연구 개요

### 연구 목적
강화학습 26강 DDPG 강의를 기반으로 DQN과 DDPG 알고리즘의 **결정적(deterministic) 정책** 측면을 중심으로 한 포괄적 비교분석 수행

### 연구 배경
교수님께서 요구하신 **코드 구현을 통한 두 알고리즘의 차이점** 명확화, 특히 결정적 정책의 구현 방식과 행동 공간 처리 방법의 본질적 차이 분석

### 연구 범위
- **핵심 주제**: 결정적 정책의 암묵적 vs 명시적 구현
- **기술적 범위**: 이산/연속 행동 공간, 액터-크리틱 구조, 경험 리플레이, 타겟 네트워크
- **실험적 범위**: CartPole-v1 (DQN) vs Pendulum-v1 (DDPG) 환경

## 🎯 연구 목표

### 1. 결정적 정책 구현 방식 비교
- **DQN**: 암묵적 결정적 정책 (Q-값 → argmax)
- **DDPG**: 명시적 결정적 정책 (액터 직접 출력)

### 2. 행동 공간 처리 메커니즘 분석
- **이산적 행동 공간**: DQN의 argmax 기반 선택
- **연속적 행동 공간**: DDPG의 직접 생성

### 3. 액터-크리틱 vs 가치 기반 구조 분석
- **DQN**: 단일 Q-네트워크 구조
- **DDPG**: 액터-크리틱 이중 네트워크 구조

### 4. 탐험 전략 및 안정성 기법 비교
- **탐험**: ε-greedy vs 가우시안 노이즈
- **안정성**: 공통 기법(경험 리플레이, 타겟 네트워크)의 각기 다른 적용

## 🔬 연구 방법론

### 1. 이론적 분석 프레임워크

#### 1.1 결정적 정책 이론 분석
```
DQN: π(s) = argmax_a Q(s,a)  (암묵적)
DDPG: π(s) = μ(s)           (명시적)
```

#### 1.2 행동 공간별 최적화 전략
- **이산**: 유한한 행동 집합에서의 최적 선택
- **연속**: 무한한 행동 공간에서의 최적 생성

### 2. 코드 구현 기반 실증 분석

#### 2.1 네트워크 아키텍처 비교
```python
# DQN: 상태 → Q-값들
q_values = self.q_network(state)
action = q_values.argmax()

# DDPG: 상태 → 직접 행동
action = self.actor_network(state)
```

#### 2.2 학습 알고리즘 차이 분석
```python
# DQN: TD 학습
target = reward + gamma * target_q_network(next_state).max()

# DDPG: 정책 경사
actor_loss = -critic_network(state, actor_network(state)).mean()
```

### 3. 실험적 검증 방법

#### 3.1 정량적 성능 평가
- 에피소드 보상 비교
- 학습 안정성 분석
- 수렴 속도 측정

#### 3.2 정성적 행동 분석
- 결정적 정책 일관성 테스트
- 행동 선택 분포 시각화
- 탐험-활용 트레이드오프 분석

## 📊 분석 도구 및 시각화

### 1. 핵심 분석 도구
- **실시간 그래프 시각화**: 학습 과정 모니터링
- **종합 시각화 시스템**: 게임플레이 + 그래프 + 통계
- **결정적 정책 특성 분석**: Q-값 분포, 액터 출력 일관성

### 2. 시각화 컴포넌트
```
ComprehensiveVisualizationWrapper:
├── 게임플레이 영상 (상단)
├── 실시간 학습 그래프 (하단 좌측)
└── 성능 통계 메트릭 (하단 우측)
```

### 3. 분석 메트릭
- **DQN**: Q-값 차이, 행동 선택 분포, ε-decay 효과
- **DDPG**: 액터 출력 분산, 노이즈 영향, 정책 일관성

## 🧪 실험 설계

### 1. 환경 설정
```yaml
DQN:
  환경: CartPole-v1
  행동공간: Discrete(2)  # [좌, 우]
  상태공간: Box(4,)      # [위치, 속도, 각도, 각속도]

DDPG:
  환경: Pendulum-v1
  행동공간: Box(1,)      # [-2.0, 2.0] 토크
  상태공간: Box(3,)      # [cos(θ), sin(θ), θ_dot]
```

### 2. 하이퍼파라미터 설정
```yaml
공통:
  학습률: 0.001
  배치크기: 32
  버퍼크기: 100000
  타겟업데이트: soft (DDPG), hard (DQN)

DQN특화:
  epsilon_decay: 0.995
  epsilon_min: 0.01

DDPG특화:
  tau: 0.005  # soft update
  noise_std: 0.1
```

### 3. 실험 프로토콜
1. **기본 학습**: 각 알고리즘 500 에피소드 훈련
2. **성능 평가**: 100 에피소드 평가 실행
3. **결정적 특성 검증**: 동일 상태 반복 테스트
4. **비교 분석**: 시각화 및 통계적 비교

## 📈 예상 결과 및 분석 포인트

### 1. 결정적 정책 구현의 차이점
- **DQN**: Q-값 계산 → argmax를 통한 간접적 결정
- **DDPG**: 신경망 직접 출력을 통한 명시적 결정

### 2. 행동 공간 적응성
- **이산 환경**: DQN의 자연스러운 적합성
- **연속 환경**: DDPG의 필수적 역할

### 3. 탐험 전략의 효과
- **ε-greedy**: 이산 행동에서의 단순하고 효과적인 탐험
- **가우시안 노이즈**: 연속 행동에서의 세밀한 탐험

### 4. 학습 안정성 및 수렴성
- **DQN**: 상대적으로 안정적인 수렴
- **DDPG**: 초기 불안정성이지만 연속 제어에서의 우수한 성능

## 📝 리포트 구성 계획

### 1. 서론 (15%)
- 연구 배경 및 동기
- 결정적 정책의 중요성
- 연구 목표 및 기여점

### 2. 이론적 배경 (25%)
- DQN 알고리즘 상세 분석
- DDPG 알고리즘 상세 분석
- 결정적 정책 이론 고찰

### 3. 구현 및 실험 (35%)
- 코드 구현 상세 설명
- 네트워크 아키텍처 비교
- 실험 설계 및 환경 설정

### 4. 결과 분석 (20%)
- 정량적 성능 비교
- 정성적 행동 분석
- 결정적 정책 특성 검증

### 5. 결론 및 의의 (5%)
- 연구 결과 요약
- 교육적 시사점
- 향후 연구 방향

## 🔧 기술적 구현 사항

### 1. 완성된 구현 요소
- ✅ DQN/DDPG 알고리즘 완전 구현
- ✅ 결정적 정책 차이점 코드 구현
- ✅ 실시간 시각화 시스템
- ✅ 종합 비교 분석 도구
- ✅ 성능 평가 메트릭

### 2. 분석 도구
- ✅ `visualizations.py`: 학습 곡선, 정책 분석, 성능 비교
- ✅ `comprehensive_visualization_wrapper.py`: 실시간 종합 시각화
- ✅ `run_experiment.py`: 자동화된 실험 실행

### 3. 데이터 수집
- ✅ 학습 메트릭 자동 수집
- ✅ JSON 형태 결과 저장
- ✅ 시각화 자동 생성

## 🎓 교육적 가치 및 기대 효과

### 1. 이론과 실습의 연계
- 강화학습 이론의 실제 구현 경험
- 알고리즘 간 차이점의 직관적 이해
- 결정적 정책 개념의 구체적 체험

### 2. 프로그래밍 역량 향상
- 딥러닝 프레임워크 활용 능력
- 복잡한 시스템 설계 및 구현
- 실험 설계 및 결과 분석 능력

### 3. 연구 방법론 학습
- 가설 설정 및 검증 과정
- 정량적/정성적 분석 방법
- 학술적 리포트 작성 기법

## 🚀 실행 계획

### 1. 1단계: 이론 정리 및 코드 검토 (완료)
- ✅ 기존 구현 코드 분석
- ✅ 이론적 배경 문서화
- ✅ 실험 환경 설정

### 2. 2단계: 실험 실행 및 데이터 수집
```bash
# 실험 실행
python run_experiment.py --save-models --results-dir results

# 시각화 생성
python render_learning_video.py --sample-data --all
```

### 3. 3단계: 결과 분석 및 시각화
```bash
# 결정적 정책 분석
python experiments/analyze_deterministic_policy.py

# 비교 분석 리포트 생성
python experiments/generate_comparison_report.py
```

### 4. 4단계: 리포트 작성 및 완성
- 실험 결과 종합 분석
- 교육적 해석 및 시사점 도출
- 최종 리포트 완성

---

**본 연구계획서는 강화학습 26강 DDPG 강의를 바탕으로 DQN과 DDPG의 결정적 정책 특성을 코드 구현을 통해 명확히 비교분석하여, 두 알고리즘의 본질적 차이점을 이해하고 각각의 적용 영역과 한계를 파악하는 것을 목표로 합니다.**