# DQN vs DDPG 알고리즘 이론 분석

## 📌 프로젝트 배경

강화학습 26강 DDPG 강의를 바탕으로 DQN과 DDPG를 비교 분석하는 프로젝트입니다. 교수님께서 **코드 구현을 통해 두 알고리즘의 차이점, 특히 "결정적(deterministic)" 측면**을 명확히 보여주는 리포트 작성을 요구하였습니다.

## 🧠 알고리즘 심층 분석

### 1. DQN (Deep Q-Network)

#### 핵심 개념
DQN은 **밸류 함수 기반** 딥 강화학습 알고리즘의 대표 모델입니다. 기존 Q-러닝에 딥러닝을 접목하여 대규모 상태 공간을 처리할 수 있도록 확장했습니다.

#### 주요 작동 메커니즘

**1. Q-테이블의 신경망 근사**
- 기존 Q-테이블을 딥 뉴럴 네트워크로 근사
- 아타리 게임: 화면 이미지 → 각 행동의 Q-값
- CNN 활용으로 고차원 상태 공간(84×84×4 픽셀) → 압축된 피처

**2. 경험 리플레이 (Experience Replay)**
- **목적**: 시간적 상관관계(temporal correlations) 문제 해결
- **방법**: 경험 (s, a, r, s', done)을 리플레이 버퍼에 저장
- **효과**: 미니배치 무작위 샘플링으로 안정적 학습

**3. 타겟 네트워크 (Target Network)**
- **목적**: 불안정한 타겟 문제(non-stationary targets) 해결
- **구조**: Q 네트워크와 동일한 구조의 별도 네트워크 Q̂
- **업데이트**: 주기적으로만 메인 네트워크를 복사

#### 한계점
- **이산적 행동 공간**에서만 작동
- 연속적 행동 공간에는 적용 어려움
- argmax 연산이 이산 행동에만 가능

### 2. DDPG (Deep Deterministic Policy Gradient)

#### 핵심 개념
DDPG는 **연속적 행동 공간** 문제 해결을 위해 고안된 알고리즘입니다. **결정적 정책 경사(Deterministic Policy Gradient)** 이론 기반의 **액터-크리틱** 방법론입니다.

#### 주요 작동 메커니즘

**1. 액터-크리틱 구조**
- **액터 네트워크 (μ)**: 
  - 정책 담당
  - 상태 s → **결정적 행동 a = μ(s)** 직접 출력
  - 확률적 정책과 대조되는 결정적 특성
- **크리틱 네트워크 (Q)**:
  - 가치 함수 담당
  - 상태-행동 쌍 (s,a) → Q-값 평가
  - DQN과 유사하지만 타겟 계산에 타겟 액터 사용

**2. 연속 행동 공간 처리**
- 액터가 행동을 직접 출력하여 연속 제어 가능
- 로봇 관절 각도, 차량 스티어링 등에 효과적
- 무한한 행동 가능성을 직접 생성

**3. DQN 혁신 기법 채용**
- **경험 리플레이**: 안정적 학습을 위해 도입
- **타겟 네트워크**: 액터와 크리틱 모두에 적용
- 기존 DQN의 성공 요소를 연속 도메인으로 확장

**4. 탐험 메커니즘**
- 결정적 정책으로 인한 탐험 부족 문제
- 액터 출력에 노이즈 추가로 해결
- Ornstein-Uhlenbeck 또는 가우시안 노이즈 사용

#### 성능 특성
- 훈련 과정에서 단조적 향상 보장 없음
- 초기 학습 불안정성 가능
- 하이퍼파라미터에 민감

## 🔍 핵심 비교 분석: "결정적(Deterministic)" 정책

### 근본적 차이점

**DQN**: 간접적 최적 행동 **선택** (Q-함수를 통해)
**DDPG**: 직접적 최적 행동 **생성** (액터를 통해)

### 상세 비교표

| 구분 | DQN (Deep Q-Network) | DDPG (Deep Deterministic Policy Gradient) |
|:-----|:--------------------|:--------------------------------------------|
| **알고리즘 유형** | 가치 기반(Value-based) DRL | 정책 기반(Policy-based) DRL (액터-크리틱) |
| **목표** | 최적 Q-함수 Q*(s,a) 학습 | 최적 결정적 정책 a = μ*(s) 및 해당 Q-함수 학습 |
| **정책 유형** | **암묵적(Implicit) 정책**<br>Q-값으로부터 행동 선택<br>`argmax Q(s,a)` 기반 **결정적 정책**<br>탐험용 ε-greedy 사용 | **명시적(Explicit) 정책**<br>액터 네트워크가 직접 행동 출력<br>**결정적 정책 a = μ(s)** 학습 |
| **행동 공간** | **이산적(Discrete)** 행동 공간만 처리 가능 | **연속적(Continuous)** 행동 공간 처리 가능 |
| **탐험 전략** | ε-greedy (확률 ε로 무작위 행동) | 액터 출력에 노이즈 추가 (OU/가우시안) |
| **안정성 보장** | 타겟 네트워크 + 경험 리플레이 | 타겟 네트워크 + 경험 리플레이 (DQN 채용) |
| **단조적 개선** | 일반적으로 안정적 | 단조적 개선 보장 없음 |

### 결정적 정책의 심화 분석

#### DQN의 암묵적 결정적 정책
- **메커니즘**: Q(s,a) 값 학습 → argmax 연산으로 최적 행동 선택
- **수식**: `a* = argmax_a Q(s,a)`
- **특징**: 
  - 각 상태에 대해 하나의 최적 행동을 암시적으로 지정
  - 이산 행동에서 argmax 연산이 용이
  - 탐험을 위해 ε-greedy 사용하지만 학습된 정책 자체는 결정적

#### DDPG의 명시적 결정적 정책
- **메커니즘**: 액터 네트워크가 상태 입력 시 정확히 하나의 행동 출력
- **수식**: `a = μ(s)`
- **특징**:
  - 연속적 행동을 직접 생성
  - 로봇 관절 각도(35.7도) 같은 정밀한 연속 제어
  - 모든 가능한 연속 행동에 대한 Q-값 계산이 불가능한 문제 해결
  - 노이즈 추가로 탐험 구현

## 💻 코드 구현 가이드

### 1. 모델 아키텍처 및 행동 공간 정의

#### DQN 구현
```python
# Q-네트워크: 이산 행동 개수만큼 출력 노드
self.q_values = nn.Linear(hidden_size, num_actions)

# 각 노드 = 해당 행동의 Q-값
```

#### DDPG 구현
```python
# 액터 네트워크: 연속 행동 변수 개수만큼 출력
self.action_output = nn.Linear(hidden_size, action_dimension)
# tanh 활성화로 행동 범위 조절

# 크리틱 네트워크: 상태+행동 입력
self.q_value_output = nn.Linear(hidden_state_size, 1)
```

### 2. 행동 선택(Action Selection) 로직

#### DQN: ε-greedy + argmax
```python
if random.random() < epsilon:
    action = random.randrange(num_actions)  # 탐험
else:
    q_values = self.q_network(state)
    action = q_values.argmax().item()  # 결정적 선택
```

#### DDPG: 액터 출력 + 노이즈
```python
action = self.actor_network(state).detach().cpu().numpy()
noise = self.noise_process.sample()  # OU 또는 가우시안
action = action + noise
action = np.clip(action, action_low, action_high)
```

### 3. 네트워크 학습(Network Update) 로직

#### DQN: TD 타겟 + MSE 손실
```python
td_target = reward + self.gamma * self.target_q_network(next_state).max(1).detach()
loss = F.mse_loss(q_values, td_target)
```

#### DDPG: 액터-크리틱 분리 학습
```python
# 크리틱 업데이트
target_q_value = reward + self.gamma * self.target_critic_network(
    next_state, self.target_actor_network(next_state)).detach()
critic_loss = F.mse_loss(current_q_value, target_q_value)

# 액터 업데이트 (결정적 정책 경사)
actor_loss = -self.critic_network(state, self.actor_network(state)).mean()
```

## 🎯 구현 완성도 검증

본 프로젝트에서는 위의 모든 이론적 요구사항이 다음과 같이 완벽 구현되었습니다:

### ✅ 완성된 구현 요소들
1. **결정적 정책 차이**: 코드로 명확히 구현 및 비교
2. **모델 아키텍처**: DQN/DDPG 네트워크 구조 정확히 구현
3. **행동 선택 로직**: ε-greedy vs 노이즈 추가 방식 구현
4. **학습 알고리즘**: TD 학습 vs 정책 경사 구현
5. **안정성 기법**: 경험 리플레이, 타겟 네트워크 구현
6. **교육적 시각화**: 결정적 정책 특성을 명확히 보여주는 실험

### 🏆 교육적 가치

이 분석을 통해 다음을 명확히 이해할 수 있습니다:

1. **정책 표현의 본질적 차이**:
   - "어떤 행동이 가장 좋은가?" (DQN) vs "이 상황에서 어떤 행동을 할까?" (DDPG)

2. **행동 공간이 알고리즘 설계에 미치는 영향**:
   - 이산적: 열거 가능 → argmax 연산
   - 연속적: 무한 가능성 → 직접 생성

3. **결정적 정책의 구현 방식**:
   - 간접적 구현 (value → action)
   - 직접적 구현 (state → action)

이러한 심층 분석을 통해 강화학습 알고리즘의 핵심 설계 원리와 각 접근법의 장단점을 실습으로 체험할 수 있습니다.