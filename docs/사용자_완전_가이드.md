# 🎯 DQN vs DDPG 사용자 완전 가이드

> **한 번에 모든 것을 해결하는 완전한 사용자 가이드**  
> 설치부터 고급 활용까지 모든 내용을 포함합니다.

## 📋 목차

1. [빠른 시작](#1-빠른-시작)
2. [프로젝트 이해](#2-프로젝트-이해)
3. [상세 사용법](#3-상세-사용법)
4. [고급 활용](#4-고급-활용)
5. [문제 해결](#5-문제-해결)

---

## 1. 🚀 빠른 시작

### 1.1 필수 준비 사항

**시스템 요구사항:**
- Windows 10/11, macOS, Linux
- Python 3.8 이상
- Git

**권장 환경:**
- Conda 또는 Python 가상환경
- 8GB+ RAM
- GPU (선택사항, 학습 속도 향상)

### 1.2 30초 설치

```bash
# 1. 프로젝트 다운로드
git clone <repository-url>
cd dqn,ddpg

# 2. 가상환경 생성 (권장)
conda create -n dqn_ddpg python=3.11
conda activate dqn_ddpg

# 3. 의존성 설치
pip install -r requirements.txt

# 4. 테스트 실행
python tests/simple_demo.py
```

### 1.3 첫 실행 (3가지 방법)

#### 🎬 방법 1: 비디오 데모 (가장 빠름)
```bash
python quick_video_demo.py --duration 15
# 결과: videos/quick_demo.mp4 생성 (15초 영상)
```

#### 🤖 방법 2: 결정적 정책 시연
```bash
python tests/simple_demo.py
# 결과: DQN과 DDPG의 정책 차이 콘솔 출력
```

#### 🏃 방법 3: 기본 학습 실행
```bash
python simple_training.py
# 결과: 간단한 학습 및 결과 분석
```

---

## 2. 🧠 프로젝트 이해

### 2.1 핵심 개념

이 프로젝트는 **결정적 정책(Deterministic Policy)**을 구현하는 두 가지 방법을 비교합니다:

| 특성 | DQN | DDPG |
|------|-----|------|
| **정책 유형** | 암묵적 결정적 | 명시적 결정적 |
| **구현 방법** | `Q(s).argmax()` | `actor(s)` |
| **행동 공간** | 이산적 (CartPole) | 연속적 (Pendulum) |
| **탐험 방식** | ε-greedy | 가우시안 노이즈 |
| **네트워크** | Q-네트워크 | 액터-크리틱 |

### 2.2 교육적 가치

#### DQN의 암묵적 결정적 정책
```python
# Q-값을 계산한 후 최댓값 선택
q_values = q_network(state)
action = q_values.argmax()  # 결정적이지만 간접적
```

#### DDPG의 명시적 결정적 정책
```python
# 액터가 직접 행동 출력
action = actor_network(state)  # 결정적이고 직접적
```

### 2.3 프로젝트 구조

```
📁 핵심 구성 요소
├── src/agents/          # 완전히 구현된 DQN, DDPG
├── src/networks/        # Q-네트워크, 액터-크리틱
├── src/core/           # 공통 유틸리티
├── configs/            # 설정 파일들
├── tests/              # 테스트 및 데모
└── videos/             # 생성된 영상들
```

---

## 3. 📖 상세 사용법

### 3.1 기본 학습 실행

#### 간단한 학습
```bash
python simple_training.py
```

#### 전체 실험 실행
```bash
python run_experiment.py --save-models --results-dir results
```

#### 비디오 녹화와 함께 학습
```bash
python run_experiment.py --record-video --dual-video
```

### 3.2 설정 커스터마이징

#### DQN 설정 (`configs/dqn_config.yaml`)
```yaml
dqn:
  learning_rate: 0.001
  epsilon_start: 1.0
  epsilon_end: 0.01
  epsilon_decay: 0.995
  batch_size: 32
  buffer_size: 10000
```

#### DDPG 설정 (`configs/ddpg_config.yaml`)
```yaml
ddpg:
  actor_lr: 0.001
  critic_lr: 0.002
  tau: 0.005  # soft update parameter
  batch_size: 64
  buffer_size: 100000
```

### 3.3 실험 분석

#### 학습 결과 확인
```bash
# 결과 디렉토리 구조
results/
├── dqn_results.json           # DQN 학습 메트릭
├── ddpg_results.json          # DDPG 학습 메트릭
├── models/                    # 저장된 모델들
└── deterministic_policy_analysis.png  # 정책 분석 그래프
```

#### 시각화 실행
```bash
python experiments/visualizations.py
```

---

## 4. 🎬 고급 활용

### 4.1 비디오 생성

#### 빠른 데모 영상
```bash
# 15초 데모
python quick_video_demo.py --duration 15

# 고화질 긴 영상
python quick_video_demo.py --duration 60 --fps 30 --output hd_demo.mp4
```

#### 학습 과정 애니메이션
```bash
# 샘플 데이터로 테스트
python render_learning_video.py --sample-data --learning-only

# 실제 학습 결과 사용
python render_learning_video.py \
    --dqn-results results/dqn_results.json \
    --ddpg-results results/ddpg_results.json \
    --all
```

#### 게임플레이 비교 영상
```bash
# 자동으로 최고 성능 에피소드 비교
python create_comparison_video.py --auto

# 특정 영상 비교
python create_comparison_video.py \
    --dqn-video videos/dqn/highlights/episode_300.mp4 \
    --ddpg-video videos/ddpg/highlights/episode_200.mp4
```

### 4.2 배치 실행 (Windows)

#### 전체 파이프라인 자동화
```batch
@echo off
echo 가상환경 활성화 중...
call conda activate dqn_ddpg

echo DQN 학습 시작...
python run_experiment.py --algorithm dqn --episodes 500 --record-video

echo DDPG 학습 시작...
python run_experiment.py --algorithm ddpg --episodes 400 --record-video

echo 비교 영상 생성...
python create_comparison_video.py --auto

echo 학습 과정 영상 생성...
python render_learning_video.py --auto-detect

echo 완료! 결과를 확인하세요.
pause
```

### 4.3 커스텀 실험

#### 새로운 환경 추가
```python
# src/environments/custom_env.py
class CustomEnvironment:
    def __init__(self, env_name):
        self.env = gym.make(env_name)
        # 커스텀 로직 추가
```

#### 하이퍼파라미터 튜닝
```bash
# 여러 학습률로 실험
for lr in 0.001 0.005 0.01; do
    python run_experiment.py --learning-rate $lr --save-dir results_lr_$lr
done
```

---

## 5. 🔧 문제 해결

### 5.1 설치 문제

#### PyTorch 설치 실패
```bash
# CPU 버전
pip install torch torchvision torchaudio

# GPU 버전 (CUDA 11.8)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### Gymnasium 호환성 문제
```bash
# 특정 버전 설치
pip install gymnasium==0.29.0
```

### 5.2 실행 문제

#### 의존성 오류
```bash
# 문제: ModuleNotFoundError
# 해결: 가상환경 확인 및 재설치
conda activate dqn_ddpg
pip install -r requirements.txt --force-reinstall
```

#### 메모리 부족
```bash
# configs/dqn_config.yaml에서 배치 크기 조정
dqn:
  batch_size: 16  # 32에서 16으로 감소
  buffer_size: 5000  # 10000에서 5000으로 감소
```

### 5.3 비디오 생성 문제

#### FFmpeg 없음 경고
```
[WARNING] ffmpeg을 사용할 수 없습니다.
```
**해결**: OpenCV 백업 시스템이 자동으로 작동하므로 무시해도 됩니다.

#### 한글 폰트 경고
```
Glyph missing from font(s) DejaVu Sans
```
**해결**: 영어 버전 스크립트 사용
```bash
python quick_video_demo.py  # 영어 버전
```

### 5.4 성능 문제

#### 학습 속도가 느림
```yaml
# configs/dqn_config.yaml
training:
  episodes: 100        # 500에서 100으로 감소
  update_frequency: 10  # 더 자주 업데이트
```

#### 비디오 생성 시간이 김
```bash
# 빠른 설정 사용
python quick_video_demo.py --duration 10 --fps 15
```

### 5.5 일반적인 오류들

#### 환경 생성 실패
```python
# 문제: gymnasium.error.UnregisteredEnv
# 해결: 환경 이름 확인
env = gym.make('CartPole-v1')  # CartPole-v0가 아님
```

#### 모델 저장 실패
```bash
# 결과 디렉토리 수동 생성
mkdir -p results/models
```

---

## 🎯 다음 단계

### 초보자 추천 순서
1. `python tests/simple_demo.py` - 정책 차이 이해
2. `python quick_video_demo.py` - 시각적 확인
3. `python simple_training.py` - 실제 학습 체험
4. `python run_experiment.py` - 전체 파이프라인

### 고급 사용자 추천
1. 설정 파일 커스터마이징
2. 새로운 환경 실험
3. 하이퍼파라미터 튜닝
4. 커스텀 네트워크 구조 시도

### 교육자 추천
1. 학생용 배치 스크립트 작성
2. 비디오 콘텐츠 생성
3. 이론 설명과 함께 시연
4. 과제용 변형 실험 설계

---

## 📚 추가 리소스

- **이론 분석**: `docs/알고리즘_이론_분석.md`
- **비디오 기능**: `docs/비디오_시스템_완전_가이드.md`
- **개발 로그**: `docs/개발_진행_로그.md`
- **프로젝트 요약**: `PROJECT_SUMMARY.md`

**문의사항이나 문제가 있다면 이슈를 등록해주세요!** 🚀