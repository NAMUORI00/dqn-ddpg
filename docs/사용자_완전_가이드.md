# 🎯 DQN vs DDPG 사용자 완전 가이드

> **한 번에 모든 것을 해결하는 완전한 사용자 가이드**  
> 설치부터 고급 활용까지 모든 내용을 포함합니다.

## 📋 목차

1. [빠른 시작](#1-빠른-시작)
2. [프로젝트 이해](#2-프로젝트-이해)
3. [상세 사용법](#3-상세-사용법)
4. [고급 활용](#4-고급-활용)
5. [종합 시각화 시스템](#5-종합-시각화-시스템)
6. [문제 해결](#6-문제-해결)

---

## 1. 🚀 빠른 시작

### 1.1 필수 준비 사항

**시스템 요구사항:**
- Windows 10/11, macOS, Linux
- Python 3.8 이상
- Git

**권장 환경:**
- Conda 또는 Python 가상환경
- 8GB+ RAM
- GPU (선택사항, 학습 속도 향상)

### 1.2 30초 설치

```bash
# 1. 프로젝트 다운로드
git clone <repository-url>
cd dqn,ddpg

# 2. 가상환경 생성 (권장)
conda create -n dqn_ddpg python=3.11
conda activate dqn_ddpg

# 3. 의존성 설치
pip install -r requirements.txt

# 4. 테스트 실행
python tests/simple_demo.py
```

### 1.3 첫 실행 (3가지 방법)

#### 🎬 방법 1: 비디오 데모 (가장 빠름)
```bash
python quick_video_demo.py --duration 15
# 결과: videos/quick_demo.mp4 생성 (15초 영상)
```

#### 🤖 방법 2: 결정적 정책 시연
```bash
python tests/simple_demo.py
# 결과: DQN과 DDPG의 정책 차이 콘솔 출력
```

#### 🏃 방법 3: 기본 학습 실행
```bash
python simple_training.py
# 결과: 간단한 학습 및 결과 분석
```

---

## 2. 🧠 프로젝트 이해

### 2.1 핵심 개념

이 프로젝트는 **결정적 정책(Deterministic Policy)**을 구현하는 두 가지 방법을 비교합니다:

| 특성 | DQN | DDPG |
|------|-----|------|
| **정책 유형** | 암묵적 결정적 | 명시적 결정적 |
| **구현 방법** | `Q(s).argmax()` | `actor(s)` |
| **행동 공간** | 이산적 (CartPole) | 연속적 (Pendulum) |
| **탐험 방식** | ε-greedy | 가우시안 노이즈 |
| **네트워크** | Q-네트워크 | 액터-크리틱 |

### 2.2 교육적 가치

#### DQN의 암묵적 결정적 정책
```python
# Q-값을 계산한 후 최댓값 선택
q_values = q_network(state)
action = q_values.argmax()  # 결정적이지만 간접적
```

#### DDPG의 명시적 결정적 정책
```python
# 액터가 직접 행동 출력
action = actor_network(state)  # 결정적이고 직접적
```

### 2.3 프로젝트 구조

```
📁 핵심 구성 요소
├── src/agents/          # 완전히 구현된 DQN, DDPG
├── src/networks/        # Q-네트워크, 액터-크리틱
├── src/core/           # 공통 유틸리티
├── configs/            # 설정 파일들
├── tests/              # 테스트 및 데모
└── videos/             # 생성된 영상들
```

---

## 3. 📖 상세 사용법

### 3.1 기본 학습 실행

#### 간단한 학습
```bash
python simple_training.py
```

#### 전체 실험 실행
```bash
python run_experiment.py --save-models --results-dir results
```

#### 비디오 녹화와 함께 학습
```bash
python run_experiment.py --record-video --dual-video
```

### 3.2 설정 커스터마이징

#### DQN 설정 (`configs/dqn_config.yaml`)
```yaml
dqn:
  learning_rate: 0.001
  epsilon_start: 1.0
  epsilon_end: 0.01
  epsilon_decay: 0.995
  batch_size: 32
  buffer_size: 10000
```

#### DDPG 설정 (`configs/ddpg_config.yaml`)
```yaml
ddpg:
  actor_lr: 0.001
  critic_lr: 0.002
  tau: 0.005  # soft update parameter
  batch_size: 64
  buffer_size: 100000
```

### 3.3 실험 분석

#### 학습 결과 확인
```bash
# 결과 디렉토리 구조
results/
├── dqn_results.json           # DQN 학습 메트릭
├── ddpg_results.json          # DDPG 학습 메트릭
├── models/                    # 저장된 모델들
└── deterministic_policy_analysis.png  # 정책 분석 그래프
```

#### 시각화 실행
```bash
python experiments/visualizations.py
```

---

## 4. 🎬 고급 활용

### 4.1 비디오 생성

#### 빠른 데모 영상
```bash
# 15초 데모
python quick_video_demo.py --duration 15

# 고화질 긴 영상
python quick_video_demo.py --duration 60 --fps 30 --output hd_demo.mp4
```

#### 학습 과정 애니메이션
```bash
# 샘플 데이터로 테스트
python render_learning_video.py --sample-data --learning-only

# 실제 학습 결과 사용
python render_learning_video.py \
    --dqn-results results/dqn_results.json \
    --ddpg-results results/ddpg_results.json \
    --all
```

#### 게임플레이 비교 영상
```bash
# 자동으로 최고 성능 에피소드 비교
python create_comparison_video.py --auto

# 특정 영상 비교
python create_comparison_video.py \
    --dqn-video videos/dqn/highlights/episode_300.mp4 \
    --ddpg-video videos/ddpg/highlights/episode_200.mp4
```

### 4.2 배치 실행 (Windows)

#### 전체 파이프라인 자동화
```batch
@echo off
echo 가상환경 활성화 중...
call conda activate dqn_ddpg

echo DQN 학습 시작...
python run_experiment.py --algorithm dqn --episodes 500 --record-video

echo DDPG 학습 시작...
python run_experiment.py --algorithm ddpg --episodes 400 --record-video

echo 비교 영상 생성...
python create_comparison_video.py --auto

echo 학습 과정 영상 생성...
python render_learning_video.py --auto-detect

echo 완료! 결과를 확인하세요.
pause
```

### 4.3 커스텀 실험

#### 새로운 환경 추가
```python
# src/environments/custom_env.py
class CustomEnvironment:
    def __init__(self, env_name):
        self.env = gym.make(env_name)
        # 커스텀 로직 추가
```

#### 하이퍼파라미터 튜닝
```bash
# 여러 학습률로 실험
for lr in 0.001 0.005 0.01; do
    python run_experiment.py --learning-rate $lr --save-dir results_lr_$lr
done
```

---

## 5. 🔧 문제 해결

### 5.1 설치 문제

#### PyTorch 설치 실패
```bash
# CPU 버전
pip install torch torchvision torchaudio

# GPU 버전 (CUDA 11.8)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### Gymnasium 호환성 문제
```bash
# 특정 버전 설치
pip install gymnasium==0.29.0
```

### 5.2 실행 문제

#### 의존성 오류
```bash
# 문제: ModuleNotFoundError
# 해결: 가상환경 확인 및 재설치
conda activate dqn_ddpg
pip install -r requirements.txt --force-reinstall
```

#### 메모리 부족
```bash
# configs/dqn_config.yaml에서 배치 크기 조정
dqn:
  batch_size: 16  # 32에서 16으로 감소
  buffer_size: 5000  # 10000에서 5000으로 감소
```

### 5.3 비디오 생성 문제

#### FFmpeg 없음 경고
```
[WARNING] ffmpeg을 사용할 수 없습니다.
```
**해결**: OpenCV 백업 시스템이 자동으로 작동하므로 무시해도 됩니다.

#### 한글 폰트 경고
```
Glyph missing from font(s) DejaVu Sans
```
**해결**: 영어 버전 스크립트 사용
```bash
python quick_video_demo.py  # 영어 버전
```

### 5.4 성능 문제

#### 학습 속도가 느림
```yaml
# configs/dqn_config.yaml
training:
  episodes: 100        # 500에서 100으로 감소
  update_frequency: 10  # 더 자주 업데이트
```

#### 비디오 생성 시간이 김
```bash
# 빠른 설정 사용
python quick_video_demo.py --duration 10 --fps 15
```

### 5.5 일반적인 오류들

#### 환경 생성 실패
```python
# 문제: gymnasium.error.UnregisteredEnv
# 해결: 환경 이름 확인
env = gym.make('CartPole-v1')  # CartPole-v0가 아님
```

#### 모델 저장 실패
```bash
# 결과 디렉토리 수동 생성
mkdir -p results/models
```

---

## 🎯 다음 단계

### 초보자 추천 순서
1. `python tests/simple_demo.py` - 정책 차이 이해
2. `python quick_video_demo.py` - 시각적 확인
3. `python simple_training.py` - 실제 학습 체험
4. `python run_experiment.py` - 전체 파이프라인

### 고급 사용자 추천
1. 설정 파일 커스터마이징
2. 새로운 환경 실험
3. 하이퍼파라미터 튜닝
4. 커스텀 네트워크 구조 시도

### 교육자 추천
1. 학생용 배치 스크립트 작성
2. 비디오 콘텐츠 생성
3. 이론 설명과 함께 시연
4. 과제용 변형 실험 설계

---

## 5. 🎬 종합 시각화 시스템

### 5.1 종합 시각화란?

**완전한 통합 시각화**로 게임플레이, 실시간 그래프, 성능 분석을 하나의 화면에서 동시에 관찰할 수 있습니다.

```
┌─────────────────────────────────┐
│      게임플레이 영상 (상단)        │  ← DQN vs DDPG 나란히
├─────────────────────────────────┤
│      실시간 그래프 (중단)         │  ← 4개 그래프 패널
├─────────────────────────────────┤
│      성능 통계 (하단)            │  ← 수치 지표 및 비교
└─────────────────────────────────┘
```

### 5.2 종합 시각화 생성

#### ⭐ 메인 통합 영상 생성 (권장)
```bash
# 완전한 DQN vs DDPG 통합 비교 영상
python create_comprehensive_visualization.py --mode demo
```

**결과물:**
- `videos/comprehensive_visualization/comprehensive_dqn_vs_ddpg.mp4` (2.8MB)
- 게임플레이 + 그래프 + 통계가 모두 포함된 완전한 영상

#### 개별 알고리즘 영상
```bash
# DQN 개별 종합 영상: dqn_comprehensive.mp4 (1.3MB)
# DDPG 개별 종합 영상: ddpg_comprehensive.mp4 (1.6MB)
```

#### 실제 에이전트와 함께
```bash
# 실제 학습하는 에이전트로 생성 (고품질)
python create_comprehensive_visualization.py --mode real_agents
```

### 5.3 최종 스크린샷 생성

```bash
# 종합 시각화 스크린샷 생성
python test_comprehensive_screenshot.py
```

**생성되는 스크린샷:**
- `comprehensive_comparison_final.png` - 🎯 **메인 통합 비교**
- `dqn_comprehensive_final.png` - DQN 종합 화면
- `ddpg_comprehensive_final.png` - DDPG 종합 화면
- 구성요소별 개별 추출 (게임/그래프/통계)

### 5.4 시각화 구성 요소

#### 1. 게임플레이 영상 (상단)
- **DQN**: CartPole 게임 + 실시간 오버레이
- **DDPG**: Pendulum 게임 + 실시간 오버레이
- 표시 정보: 알고리즘명, 에피소드, 스텝, 보상, Q-값, 탐색률

#### 2. 실시간 그래프 (중단)
```
┌─────────────────┬─────────────────┐
│  Episode Rewards │    Q-Values     │
│  + Moving Avg   │  (실시간 변화)   │
├─────────────────┼─────────────────┤
│ Training Loss   │ Current Episode │
│  (로그 스케일)   │  (진행 상황)     │
└─────────────────┴─────────────────┘
```

#### 3. 성능 통계 (하단)
```
┌─────────────────┬─────────────────┬─────────────────┐
│   기본 정보      │   성능 통계     │   최신 메트릭    │
├─────────────────┼─────────────────┼─────────────────┤
│ Algorithm: DQN  │ Best: 245.0     │ Q-Value: 42.31  │
│ Episodes: 25    │ Worst: 9.0      │ Loss: 1.2e-03   │
│ Total Steps: 42 │ Average: 20.1   │ Exploration: 0.01│
│ Current Ep: 25  │ Std: 15.3       │ Time: 2.3min    │
│ Current St: 9   │ Current: 9.0    │ Avg Length: 20.1│
└─────────────────┴─────────────────┴─────────────────┘
```

### 5.5 개별 시각화 구성요소 테스트

#### 기본 실시간 그래프
```bash
python test_realtime_graph.py --mode basic
```

#### 고급 메트릭 그래프
```bash
python test_realtime_graph.py --mode advanced
```

#### 실제 에이전트와 함께
```bash
python test_realtime_graph.py --mode agent
```

### 5.6 종합 시각화의 장점

#### 🎯 **완전한 통합**
- 게임플레이, 그래프, 통계를 한 화면에서 관찰
- DQN과 DDPG의 모든 차이점을 동시에 비교

#### 🔄 **실시간 모니터링**
- 학습 과정의 라이브 업데이트
- 에피소드별 성능 변화 즉시 확인

#### 📊 **포괄적 분석**
- Q-값, 손실, 탐색률 등 모든 메트릭 표시
- 통계적 지표로 정량적 비교

#### 🛠️ **교육적 가치**
- 알고리즘 차이점의 직관적 이해
- 이론과 실제의 완벽한 연결

### 5.7 활용 예시

#### 교육용
- 강의 시연: 실시간으로 알고리즘 차이 설명
- 과제 제출: 학습 과정 영상으로 결과 검증

#### 연구용
- 논문 보조자료: 실험 결과의 시각적 증명
- 알고리즘 개발: 새로운 방법론의 성능 비교

#### 프레젠테이션용
- 학회 발표: 포괄적인 결과 시연
- 기술 데모: 완성도 높은 시각화

---

## 6. 🔧 문제 해결

### 6.1 종합 시각화 관련 문제

#### pygame 오류
```bash
# 해결방법
pip install pygame
```

#### matplotlib 백엔드 오류
```python
# 스크립트 상단에 추가
import matplotlib
matplotlib.use('Agg')
```

#### 메모리 부족
- `history_length` 값을 50으로 감소
- 비디오 해상도를 낮춤 (frame_width=400)

---

## 📚 추가 리소스

- **이론 분석**: `docs/analysis_reports/알고리즘_이론_분석.md`
- **비디오 기능**: `docs/비디오_시스템_완전_가이드.md`
- **종합 시각화**: `docs/종합_시각화_시스템_가이드.md` ⭐ **NEW**
- **개발 로그**: `docs/documentation/개발_진행_로그.md`
- **프로젝트 요약**: `docs/final_reports/PROJECT_SUMMARY.md`

**문의사항이나 문제가 있다면 이슈를 등록해주세요!** 🚀