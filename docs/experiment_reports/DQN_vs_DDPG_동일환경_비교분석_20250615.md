
# 동일 환경 DQN vs DDPG 실험 리포트

## 실험 개요
- **환경**: ContinuousCartPole-v0 (CartPole 물리 + 연속 행동 공간)
- **목적**: 환경 차이를 배제한 순수 알고리즘 성능 비교
- **기간**: 각 알고리즘 500 에피소드 훈련

## 주요 결과

### 성능 비교
| 알고리즘 | 최종 점수 | 최고 점수 | 학습 안정성 |
|---------|----------|----------|-------------|
| DQN     | 498.95   | 500      | 불안정하지만 높은 최종 성능 |
| DDPG    | 37.80    | 116.1    | 매우 불안정하고 낮은 성능 |

### 결정적 정책 특성
- **DQN**: Q-value argmax 방식 (암묵적), 행동 범위 [-1.0, 0.8]
- **DDPG**: Actor 직접 출력 방식 (명시적), 행동 범위 [0.98, 1.0]
- **공통점**: 두 알고리즘 모두 완벽한 결정성 달성 (분산 = 0)

### 행동 선택 패턴
- 평균 행동 차이: 1.275 (큰 차이)
- 행동 상관관계: -0.031 (거의 무관)
- DQN은 다양한 행동, DDPG는 한쪽으로 치우친 행동

## 핵심 인사이트

1. **환경 적합성이 알고리즘 유형보다 중요**: 연속 환경이라고 해서 DDPG가 반드시 유리하지 않음
2. **이산화 전략의 효과**: DQN의 행동 이산화가 이 환경에서는 더 효과적
3. **탐험 전략의 중요성**: epsilon-greedy vs 가우시안 노이즈의 차이가 성능에 큰 영향
4. **학습 안정성**: DQN이 더 안정적이고 예측 가능한 학습 곡선

## 교육적 가치

이 실험은 다음을 보여줍니다:
- 알고리즘 선택 시 이론적 적합성뿐만 아니라 실제 성능도 고려해야 함
- 동일 조건에서의 공정한 비교의 중요성
- 결정적 정책의 다양한 구현 방식과 그 효과

## 향후 연구 방향

1. 다른 연속 제어 환경에서의 비교 실험
2. 하이퍼파라미터 튜닝을 통한 DDPG 성능 개선
3. 이산화 해상도가 DQN 성능에 미치는 영향 분석
