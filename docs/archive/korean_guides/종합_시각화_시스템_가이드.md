# 종합 시각화 시스템 완전 가이드

## 개요

이 프로젝트는 DQN과 DDPG 알고리즘의 차이점을 종합적으로 시각화하는 고급 시스템을 제공합니다. 게임플레이, 실시간 그래프, 성능 분석을 하나의 화면에서 동시에 관찰할 수 있어 두 알고리즘의 특성을 직관적으로 이해할 수 있습니다.

## 시각화 시스템 아키텍처

### 1. 계층화된 시각화 구조

```
┌─────────────────────────────────┐
│      게임플레이 영상 (상단)        │  ← 실시간 게임 화면 + 오버레이
├─────────────────────────────────┤
│      실시간 그래프 (중단)         │  ← 4개 그래프 패널
├─────────────────────────────────┤
│      성능 통계 (하단)            │  ← 수치 지표 및 비교 분석
└─────────────────────────────────┘
```

### 2. 시각화 래퍼 시스템

#### **RealtimeGraphWrapper** (기본 실시간 그래프)
- 에피소드별 보상 추이
- 이동평균선 표시
- 현재 에피소드 진행상황

#### **AdvancedMetricsWrapper** (고급 메트릭)
- Q-값 변화 그래프
- 학습 손실 그래프 (로그 스케일)
- 탐색률 표시
- 외부 메트릭 업데이트 지원

#### **ComprehensiveVisualizationWrapper** (종합 시각화)
- 모든 시각화 요소 통합
- 게임플레이 + 그래프 + 통계
- DQN vs DDPG 나란히 비교

## 파일 구조 및 저장 위치

### 📁 종합 시각화 출력 구조

```
videos/
├── comprehensive_visualization/              # 🌟 메인 종합 시각화
│   ├── comprehensive_dqn_vs_ddpg.mp4        # ⭐ 완전 통합 비교 영상 (2.8MB)
│   ├── dqn_comprehensive.mp4                # DQN 개별 종합 영상 (1.3MB)  
│   ├── ddpg_comprehensive.mp4               # DDPG 개별 종합 영상 (1.6MB)
│   └── final_screenshots/                   # 최종 스크린샷 모음
│       ├── comprehensive_comparison_final.png    # 🎯 최종 통합 비교
│       ├── dqn_comprehensive_final.png           # DQN 종합 스크린샷
│       ├── ddpg_comprehensive_final.png          # DDPG 종합 스크린샷
│       ├── dqn_game_only.png                     # 구성요소별 추출
│       ├── dqn_graphs_only.png
│       ├── dqn_stats_only.png
│       ├── ddpg_game_only.png
│       ├── ddpg_graphs_only.png
│       └── ddpg_stats_only.png
│
├── realtime_graph_test/                     # 실시간 그래프 테스트
│   ├── dqn_vs_ddpg_graphs.mp4
│   ├── realtime_graph_demo.mp4
│   └── screenshots/
│
├── comparison/                              # 기존 게임플레이 비교
│   ├── comparison_best_1.mp4
│   ├── comparison_best_2.mp4
│   └── comparison_early_vs_late.mp4
│
└── [기타 개별 알고리즘 영상들...]
```

## 시각화 구성 요소 상세

### 1. 🎮 게임플레이 영상 (상단 영역)

#### DQN (CartPole-v1)
```
Algorithm: DQN
Episode: 25
Steps: 156
Reward: 156.0
Q-Value: 42.31
Exploration: 0.012
```

#### DDPG (Pendulum-v1)  
```
Algorithm: DDPG
Episode: 25
Steps: 200
Reward: -892.5
Q-Value: -88.15
```

**특징:**
- 실시간 오버레이 정보 표시
- 각 알고리즘의 게임 환경 차이 시각화
- 행동 공간의 차이 (이산 vs 연속) 관찰 가능

### 2. 📊 실시간 그래프 (중단 영역)

#### 2x2 그래프 레이아웃

```
┌─────────────────┬─────────────────┐
│  Episode Rewards │    Q-Values     │
│  + Moving Avg   │  (실시간 변화)   │
├─────────────────┼─────────────────┤
│ Training Loss   │ Current Episode │
│  (로그 스케일)   │  (진행 상황)     │
└─────────────────┴─────────────────┘
```

**그래프별 세부 사항:**

1. **Episode Rewards**
   - 파란선: 실제 에피소드 보상
   - 빨간선: 이동평균 (10 에피소드)
   - DQN: 0~500 범위 (양수)
   - DDPG: -2000~0 범위 (음수)

2. **Q-Values**  
   - 보라색: Q-값 변화 추이
   - 빨간 점선: 최신 Q-값 표시
   - DQN: 점진적 증가 패턴
   - DDPG: 음수에서 점진적 개선

3. **Training Loss**
   - 주황색: 로그 스케일 손실
   - DQN: 빠른 감소 후 안정화
   - DDPG: 더 천천히 감소

4. **Current Episode Progress**
   - 초록색: 현재 에피소드 누적 보상
   - 실시간 진행상황 표시

### 3. 📈 성능 통계 (하단 영역)

#### 3열 레이아웃

```
┌─────────────────┬─────────────────┬─────────────────┐
│   기본 정보      │   성능 통계     │   최신 메트릭    │
├─────────────────┼─────────────────┼─────────────────┤
│ Algorithm: DQN  │ Best: 245.0     │ Q-Value: 42.31  │
│ Episodes: 25    │ Worst: 9.0      │ Loss: 1.2e-03   │
│ Total Steps: 42 │ Average: 20.1   │ Exploration: 0.01│
│ Current Ep: 25  │ Std: 15.3       │ Time: 2.3min    │
│ Current St: 9   │ Current: 9.0    │ Avg Length: 20.1│
└─────────────────┴─────────────────┴─────────────────┘
```

## 사용 방법

### 1. 종합 시각화 영상 생성

```bash
# 기본 데모 영상 생성 (권장)
python create_comprehensive_visualization.py --mode demo

# 실제 에이전트와 함께 생성
python create_comprehensive_visualization.py --mode real_agents

# 둘 다 생성
python create_comprehensive_visualization.py --mode both
```

### 2. 스크린샷만 생성

```bash
# 종합 시각화 스크린샷 생성
python test_comprehensive_screenshot.py
```

### 3. 개별 구성요소 테스트

```bash
# 기본 실시간 그래프
python test_realtime_graph.py --mode basic

# 고급 메트릭 그래프  
python test_realtime_graph.py --mode advanced

# 실제 에이전트와 함께
python test_realtime_graph.py --mode agent
```

## 알고리즘별 시각화 특성 비교

### DQN 특성

**게임 환경:**
- CartPole-v1 (막대 균형 맞추기)
- 이산 행동 공간 (좌/우 2개)
- 에피소드당 최대 500 스텝

**성능 패턴:**
- 빠른 학습 시작
- 높은 변동성
- Q-값 빠른 증가
- 탐색률 선형 감소

**시각적 특징:**
- 보상 그래프: 급격한 변화
- Q-값: 양수 영역에서 증가
- 손실: 빠른 수렴

### DDPG 특성

**게임 환경:**
- Pendulum-v1 (진자 제어)
- 연속 행동 공간 (토크 -2~+2)
- 에피소드당 200 스텝

**성능 패턴:**
- 천천히 학습 시작  
- 부드러운 학습 곡선
- Q-값 음수에서 점진적 개선
- 노이즈 기반 탐색

**시각적 특징:**
- 보상 그래프: 완만한 개선
- Q-값: 음수 영역에서 상승
- 손실: 안정적 감소

## 고급 기능

### 1. 실시간 메트릭 업데이트

```python
# 환경 생성
env = ComprehensiveVisualizationWrapper(
    gym.make('CartPole-v1'),
    algorithm_name="DQN"
)

# 학습 중 메트릭 업데이트
env.update_metrics(
    q_value=max_q_value,
    loss=training_loss,
    exploration_rate=epsilon
)
```

### 2. 나란히 비교 시각화

```python
# 두 알고리즘 동시 비교
combined_frame = create_side_by_side_comparison(
    dqn_wrapper, ddpg_wrapper
)
```

### 3. 구성요소별 추출

- 게임 화면만: `frame[:400, :, :]`
- 그래프만: `frame[400:700, :, :]`  
- 통계만: `frame[700:, :, :]`

## 설정 옵션

### ComprehensiveVisualizationWrapper 매개변수

```python
ComprehensiveVisualizationWrapper(
    env,
    algorithm_name="DQN",           # 알고리즘 이름
    frame_width=600,                # 게임 화면 너비  
    frame_height=400,               # 게임 화면 높이
    graph_height=300,               # 그래프 영역 높이
    stats_height=150                # 통계 영역 높이
)
```

### 그래프 설정

- `history_length=100`: 표시할 에피소드 히스토리
- `ma_window=10`: 이동평균 윈도우 크기
- `show_current_episode=True`: 현재 에피소드 표시 여부

## 성능 최적화

### 1. 비디오 품질 설정

```python
# 고품질 설정 (파일 크기 증가)
fourcc = cv2.VideoWriter_fourcc(*'H264')
fps = 60

# 표준 설정 (권장)
fourcc = cv2.VideoWriter_fourcc(*'mp4v') 
fps = 30
```

### 2. matplotlib 백엔드 최적화

```python
import matplotlib
matplotlib.use('Agg')  # GUI 없는 렌더링으로 성능 향상
```

### 3. 메모리 관리

- `deque(maxlen=N)`: 메모리 제한으로 성능 안정화
- 주기적 `plt.close()`: 메모리 누수 방지

## 문제 해결

### 일반적인 문제

1. **pygame 오류**
   ```bash
   pip install pygame
   ```

2. **matplotlib 백엔드 오류**
   ```python
   import matplotlib
   matplotlib.use('Agg')
   ```

3. **메모리 부족**
   - `history_length` 값 감소
   - 비디오 해상도 낮춤

### 디버깅 팁

1. **프레임 확인**
   ```python
   frame = wrapper.render()
   cv2.imwrite('debug_frame.png', cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))
   ```

2. **메트릭 값 출력**
   ```python
   print(f"Q-value: {wrapper.latest_q_value}")
   print(f"Loss: {wrapper.latest_loss}")
   ```

## 확장 가능성

### 1. 새로운 알고리즘 추가

- `ComprehensiveVisualizationWrapper` 상속
- 알고리즘별 특성 반영
- 메트릭 업데이트 로직 수정

### 2. 추가 그래프 패널

- `_setup_matplotlib()` 수정
- 새로운 서브플롯 추가
- 렌더링 로직 확장

### 3. 인터랙티브 기능

- 실시간 파라미터 조정
- 그래프 확대/축소
- 메트릭 선택적 표시

## 결론

이 종합 시각화 시스템은 DQN과 DDPG 알고리즘의 근본적인 차이점을 한눈에 파악할 수 있게 해주는 강력한 도구입니다. 게임플레이, 학습 과정, 성능 지표를 통합적으로 관찰함으로써 각 알고리즘의 특성과 학습 패턴을 직관적으로 이해할 수 있습니다.

**핵심 장점:**
- 🎯 **완전한 통합 시각화**: 모든 정보가 한 화면에
- 🔄 **실시간 업데이트**: 학습 과정의 라이브 모니터링  
- 📊 **포괄적 비교**: DQN vs DDPG의 모든 차이점 시각화
- 🛠️ **확장 가능**: 새로운 알고리즘 및 메트릭 추가 용이