강화 학습 26강에서 DDPG에 대한 강의를 받으셨고, 이를 바탕으로 DQN과 DDPG를 비교 분석하는 리포트를 작성하시려는군요. 교수님께서 코드 구현을 통해 두 알고리즘의 차이점, 특히 "결정적(deterministic)" 측면을 명확히 보여주는 리포트를 원하신다고 하니, 이에 초점을 맞춰 자세히 설명해 드리겠습니다.
--------------------------------------------------------------------------------
DQN(Deep Q-Network)과 DDPG(Deep Deterministic Policy Gradient) 비교 분석
DQN과 DDPG는 모두 딥 러닝을 활용하여 강화 학습 문제를 해결하는 딥 강화 학습(Deep Reinforcement Learning, DRL) 알고리즘이지만, 해결하려는 문제의 유형과 접근 방식에서 중요한 차이를 가집니다.
1. DQN(Deep Q-Network)
•
개념: DQN은 밸류 함수를 기반으로 하는 딥 강화 학습 알고리즘의 대표적인 모델입니다. 기존의 Q-러닝(Q-learning) 방식에 딥러닝의 CNN(Convolutional Neural Network)을 접목하여, 대규모의 상태 공간을 처리할 수 있도록 확장한 것입니다.
•
주요 작동 방식:
◦
Q-테이블의 근사: 기존 강화 학습에서 사용하던 Q-테이블(Q-values)을 직접 만드는 대신, 딥 뉴럴 네트워크(Q 네트워크)를 사용하여 Q-값을 근사(approximation)합니다. 예를 들어, 아타리 게임에서는 화면 이미지를 입력으로 받아 각 행동에 대한 Q-값을 출력합니다.
◦
CNN 활용: 이미지와 같이 차원이 매우 큰 상태 공간(high-dimensional state spaces)을 처리하기 위해 CNN을 사용합니다. CNN은 거대한 화면 상태(예: 84x84 픽셀 4프레임)를 256차원과 같은 작은 피처 벡터로 압축하고, 이를 기반으로 Q 함수를 학습합니다.
◦
경험 리플레이(Experience Replay): 샘플들 간의 시간적 상관관계(temporal correlations) 문제를 해결하기 위해 사용됩니다. 에이전트가 환경과 상호작용하여 얻은 경험(상태, 행동, 보상, 다음 상태)을 리플레이 버퍼(replay buffer)에 저장하고, 훈련 시에는 이 버퍼에서 미니 배치(mini-batch)를 무작위로 샘플링하여 네트워크를 업데이트합니다.
◦
타겟 네트워크(Target Network): Q-러닝의 타겟(target)이 불안정하게 변하는 문제(non-stationary targets)를 해결하기 위해 도입되었습니다. Q 네트워크와 동일한 구조를 가진 별도의 타겟 네트워크(Q̂)를 사용하여 타겟 Q-값을 계산하고, 이 타겟 네트워크는 주기적으로만 업데이트됩니다.
•
한계: DQN은 이산적인(discrete) 행동 공간에서만 작동합니다. 예를 들어, 아타리 게임처럼 조작할 수 있는 행동의 종류가 제한적일 때 유용합니다. 연속적인 행동 공간(continuous action spaces)에 대해서는 적용하기 어렵습니다.
2. DDPG(Deep Deterministic Policy Gradient)
•
개념: DDPG는 연속적인 행동 공간 문제를 해결하기 위해 고안된 딥 강화 학습 알고리즘입니다. 이는 결정적 정책 경사(Deterministic Policy Gradient, DPG) 이론을 기반으로 하는 액터-크리틱(Actor-Critic) 방법론에 속합니다.
•
주요 작동 방식:
◦
액터-크리틱 구조: DDPG는 두 개의 신경망, 즉 액터(Actor)와 크리틱(Critic)으로 구성됩니다.
▪
액터 네트워크 (µ): 정책(policy)을 담당하며, 주어진 상태(s)에서 **결정적인 행동(a = µ(s))**을 직접 출력합니다. 일반적인 정책 경사 알고리즘이 확률적 정책(stochastic policy)을 학습하는 것과 대조됩니다. 액터는 크리틱으로부터 받은 Q-값을 최대화하는 방향으로 정책을 업데이트합니다.
▪
크리틱 네트워크 (Q): 가치 함수(Q-function)를 담당하며, 주어진 상태-행동 쌍(s, a)에 대한 Q-값을 평가합니다. DQN과 유사하게 Q-값을 업데이트하지만, 타겟 Q-값을 계산할 때 타겟 액터의 정책을 사용합니다.
◦
연속 행동 공간 처리: 액터 네트워크가 행동을 직접 출력하기 때문에 로봇 관절의 각도나 차량의 스티어링 각도와 같이 연속적인 값을 갖는 행동들을 효과적으로 다룰 수 있습니다.
◦
DQN의 혁신 채용: DQN에서 성공적으로 사용된 경험 리플레이와 타겟 네트워크 개념을 DDPG도 그대로 채용하여 학습의 안정성을 높입니다.
◦
탐험(Exploration): 결정적 정책은 항상 동일한 행동을 출력하므로 탐험이 부족할 수 있습니다. 이를 보완하기 위해 액터가 출력하는 행동에 노이즈(noise)를 추가하여 탐험을 유도합니다.
•
성능 특성: DDPG의 성능은 훈련 과정에서 항상 단조적으로(monotonically) 향상된다는 보장이 없습니다.
3. DQN과 DDPG의 비교 분석 (주요 차이점 강조)
교수님께서 특히 **"결정적(deterministic)"**이라는 측면을 강조한 리포트를 원하신다고 하셨으므로, 이 부분을 명확히 설명하는 것이 중요합니다.
구분
DQN (Deep Q-Network)
DDPG (Deep Deterministic Policy Gradient)
알고리즘 유형
가치 기반(Value-based) DRL
정책 기반(Policy-based) DRL (액터-크리틱)
목표
최적 Q-함수 Q*(s,a) 학습
최적 결정적 정책 a = µ*(s) 및 해당 Q-함수 Qµ(s,a) 학습
정책 유형
암묵적(Implicit) 정책: Q-값으로부터 행동 선택.<br>주로 argmax Q(s,a) 기반의 **결정적 정책(deterministic policy)**을 지향하고, 탐험을 위해 ε-greedy를 사용해 확률적 행동을 선택.
명시적(Explicit) 정책: 액터 네트워크가 직접 행동을 출력하는 결정적 정책(deterministic policy) a = µ(s) 학습.
행동 공간
이산적(Discrete) 행동 공간만 처리 가능
연속적(Continuous) 행동 공간 처리 가능
탐험 전략
ε-greedy 정책 (확률 ε로 무작위 행동 선택)
액터 출력에 노이즈 추가 (예: Ornstein-Uhlenbeck 노이즈)
안정성 보장
타겟 네트워크 및 경험 리플레이로 안정성 확보
타겟 네트워크 및 경험 리플레이로 안정성 확보 (DQN에서 채용)
단조적 개선
-
성능의 단조적 개선이 보장되지 않음
"결정적(Deterministic)" 정책의 차이점 심화:
가장 핵심적인 차이점은 DQN이 간접적으로 최적 행동을 선택하는 반면(Q-함수를 통해), DDPG는 최적 행동을 직접적으로 생성한다는 것입니다.
•
DQN: DQN은 Q(s, a) 값을 학습합니다. 특정 상태 s에서 어떤 행동 a가 가장 높은 Q 값을 가지는지 찾아서 그 행동을 취합니다 (a* = argmax_a Q(s,a)). 이는 본질적으로 결정적인(deterministic) 행동 선택입니다. 탐험(exploration)을 위해 ε-greedy와 같은 전략을 사용하지만, 학습된 정책 자체는 각 상태에 대해 하나의 최적 행동을 암시적으로 지정합니다. 이 방식은 이산적인 행동(a0, a1, a2...) 중에서 argmax를 쉽게 찾을 수 있기 때문에 이산 행동 공간에 적합합니다.
•
DDPG: DDPG의 액터 네트워크는 상태 s가 주어졌을 때 정확히 하나의 행동 a = µ(s)를 출력합니다. 이것이 바로 **결정적 정책(deterministic policy)**입니다. 로봇의 관절 각도(예: 35.7도)와 같이 연속적인 행동을 취해야 하는 경우, DQN처럼 모든 가능한 연속적인 행동에 대한 Q-값을 계산하여 argmax를 적용하는 것은 불가능합니다. DDPG는 액터 네트워크가 직접 연속적인 행동 값을 출력함으로써 이 문제를 해결합니다. 탐험은 이 결정적 출력 값에 약간의 노이즈를 더해주는 방식으로 이루어집니다.
4. 코드 구현을 통한 차이점 강조 (리포트 구성 가이드)
교수님께서 직접 코드 구현을 통해 차이점을 보여주기를 원하신다고 하셨으니, 아래와 같은 방식으로 리포트를 구성하고 코드를 강조할 수 있습니다. (실제 코드를 제가 제공할 수는 없음을 양해 바랍니다.)
1.
모델 아키텍처 및 행동 공간 정의:
◦
DQN: Q-네트워크의 출력 레이어는 **이산적인 행동의 개수(예: 4개)**만큼의 노드(node)를 가집니다. 각 노드는 해당 행동의 Q-값을 나타냅니다.
▪
코드 예시: self.q_values = nn.Linear(hidden_size, num_actions)
◦
DDPG:
▪
액터 네트워크: 출력 레이어는 **연속적인 행동 변수의 개수(예: 로봇의 관절 4개)**만큼의 노드를 가집니다. 각 노드는 해당 행동 변수의 값을 직접 출력합니다. (예: nn.Linear(hidden_size, action_dimension) 후 활성화 함수로 tanh 등을 사용하여 행동 범위 조절)
▪
크리틱 네트워크: 입력으로 상태와 행동을 모두 받습니다 (state_dim + action_dim). 출력은 단일 Q-값입니다.
▪
코드 예시: self.action_output = nn.Linear(hidden_size, action_dimension) (액터)
▪
코드 예시: self.q_value_output = nn.Linear(hidden_state_size, 1) (크리틱)
2.
행동 선택(Action Selection) 로직:
◦
DQN: 훈련 중에는 ε-greedy 전략을 사용하고, 평가 시에는 단순히 argmax를 통해 Q-값이 가장 높은 행동을 선택합니다.
▪
코드 예시:
◦
DDPG: 액터 네트워크의 출력을 그대로 행동으로 사용하고, 탐험을 위해 이 행동에 노이즈를 추가합니다.
▪
코드 예시:
3.
네트워크 학습(Network Update) 로직:
◦
DQN: Q-네트워크의 가중치를 업데이트할 때, 타겟 Q-값을 R + gamma * max_a' Q_target(S', a')로 계산하고, 예측된 Q-값과의 MSE 손실을 최소화합니다.
▪
코드 예시: td_target = reward + self.gamma * self.target_q_network(next_state).max(1).detach()
▪
loss = F.mse_loss(q_values, td_target)
◦
DDPG:
▪
크리틱 네트워크 업데이트: 타겟 Q-값을 R + gamma * Q_target(S', Actor_target(S'))로 계산하여 MSE 손실을 최소화합니다. 여기서 Actor_target(S')는 타겟 액터가 다음 상태 S'에서 출력하는 결정적 행동입니다.
•
코드 예시: target_q_value = reward + self.gamma * self.target_critic_network(next_state, self.target_actor_network(next_state)).detach()
•
critic_loss = F.mse_loss(current_q_value, target_q_value)
▪
액터 네트워크 업데이트: 액터는 Q_critic(S, Actor(S))를 최대화하는 방향으로 업데이트됩니다. 이는 결정적 정책 경사(Deterministic Policy Gradient) 공식에 따라 계산됩니다. (소스 참고)
•
코드 예시: actor_loss = -self.critic_network(state, self.actor_network(state)).mean()
•
actor_optimizer.zero_grad(); actor_loss.backward(); actor_optimizer.step()
위의 가이드라인을 따라 코드를 구현하고, 각 코드 블록 옆에 설명과 함께 DQN과 DDPG가 어떻게 다른 행동 공간을 처리하고, 어떤 종류의 정책을 학습하며, 그로 인해 행동 선택과 학습 과정이 어떻게 달라지는지를 상세히 서술하면 교수님께서 원하시는 리포트를 완성할 수 있을 것입니다. 특히, DDPG의 "결정적 정책"이 연속적인 행동 공간 문제 해결에 어떻게 기여하는지 명확히 보여주는 데 집중하시기 바랍니다.