#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ DQN vs DDPG í•™ìŠµ ë° ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸
í•™ìŠµ, í‰ê°€, ë¹„ë””ì˜¤ ë…¹í™”ë¥¼ í¬í•¨í•œ ì „ì²´ íŒŒì´í”„ë¼ì¸
"""

import os
import yaml
import torch
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from collections import deque
import gymnasium as gym

from src.agents import DQNAgent, DDPGAgent
from src.environments.wrappers import create_dqn_env, create_ddpg_env
from src.core.utils import set_seed, get_device
from src.core.video_manager import VideoConfig, VideoManager


def load_config(config_path: str) -> dict:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def train_dqn(config: dict, episodes: int = 100) -> tuple:
    """DQN í›ˆë ¨"""
    print("\n" + "="*50)
    print("DQN í›ˆë ¨ ì‹œì‘")
    print("="*50)
    
    # í™˜ê²½ ìƒì„±
    env = create_dqn_env(config['environment']['name'])
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = DQNAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        learning_rate=config['agent']['learning_rate'],
        gamma=config['agent']['gamma'],
        epsilon=config['agent']['epsilon'],
        epsilon_min=config['agent']['epsilon_min'],
        epsilon_decay=config['agent']['epsilon_decay'],
        buffer_size=config['agent']['buffer_size'],
        batch_size=config['agent']['batch_size']
    )
    
    # í•™ìŠµ ë©”íŠ¸ë¦­
    episode_rewards = []
    episode_lengths = []
    recent_rewards = deque(maxlen=100)
    
    # í•™ìŠµ ë£¨í”„
    for episode in tqdm(range(episodes), desc="DQN Training"):
        state, _ = env.reset()
        total_reward = 0
        steps = 0
        
        while True:
            action = agent.select_action(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            agent.store_transition(state, action, reward, next_state, done)
            agent.update()
            
            state = next_state
            total_reward += reward
            steps += 1
            
            if done:
                break
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        recent_rewards.append(total_reward)
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        if episode % config['agent'].get('target_update_freq', 100) == 0:
            agent.update_target_network()
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if episode % 50 == 0:
            avg_reward = np.mean(recent_rewards) if recent_rewards else 0
            print(f"Episode {episode}, Avg Reward: {avg_reward:.2f}, Epsilon: {agent.epsilon:.3f}")
    
    env.close()
    
    return agent, {
        'episode_rewards': episode_rewards,
        'episode_lengths': episode_lengths,
        'algorithm': 'DQN'
    }


def train_ddpg(config: dict, episodes: int = 100) -> tuple:
    """DDPG í›ˆë ¨"""
    print("\n" + "="*50)
    print("DDPG í›ˆë ¨ ì‹œì‘")
    print("="*50)
    
    # í™˜ê²½ ìƒì„±
    env = create_ddpg_env(config['environment']['name'])
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = DDPGAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        actor_lr=config['agent']['actor_lr'],
        critic_lr=config['agent']['critic_lr'],
        gamma=config['agent']['gamma'],
        tau=config['agent']['tau'],
        noise_sigma=config['agent']['noise_sigma'],
        buffer_size=config['agent']['buffer_size'],
        batch_size=config['agent']['batch_size']
    )
    
    # í•™ìŠµ ë©”íŠ¸ë¦­
    episode_rewards = []
    episode_lengths = []
    recent_rewards = deque(maxlen=100)
    
    # í•™ìŠµ ë£¨í”„
    for episode in tqdm(range(episodes), desc="DDPG Training"):
        state, _ = env.reset()
        total_reward = 0
        steps = 0
        
        while True:
            action = agent.select_action(state, add_noise=True)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            agent.store_transition(state, action, reward, next_state, done)
            
            # Warmup ì´í›„ í•™ìŠµ ì‹œì‘
            if len(agent.buffer) > config['training'].get('warmup_steps', 1000):
                agent.train()
            
            state = next_state
            total_reward += reward
            steps += 1
            
            if done:
                break
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        recent_rewards.append(total_reward)
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if episode % 50 == 0:
            avg_reward = np.mean(recent_rewards) if recent_rewards else 0
            print(f"Episode {episode}, Avg Reward: {avg_reward:.2f}")
    
    env.close()
    
    return agent, {
        'episode_rewards': episode_rewards,
        'episode_lengths': episode_lengths,
        'algorithm': 'DDPG'
    }


def evaluate_agent(agent, env_name: str, episodes: int = 10, render: bool = False) -> dict:
    """ì—ì´ì „íŠ¸ í‰ê°€"""
    if hasattr(agent, 'select_action'):  # DQNì¸ì§€ í™•ì¸
        env = create_dqn_env(env_name)
        is_dqn = True
    else:
        env = create_ddpg_env(env_name)
        is_dqn = False
    
    rewards = []
    lengths = []
    
    for episode in range(episodes):
        state, _ = env.reset()
        total_reward = 0
        steps = 0
        
        while True:
            if is_dqn:
                action = agent.select_action(state, deterministic=True)
            else:
                action = agent.select_action(state, add_noise=False)
            
            state, reward, terminated, truncated, _ = env.step(action)
            total_reward += reward
            steps += 1
            
            if terminated or truncated:
                break
        
        rewards.append(total_reward)
        lengths.append(steps)
    
    env.close()
    
    return {
        'mean_reward': np.mean(rewards),
        'std_reward': np.std(rewards),
        'mean_length': np.mean(lengths),
        'rewards': rewards
    }


def create_video(agent, env_name: str, filename: str, episodes: int = 3):
    """ì—ì´ì „íŠ¸ í–‰ë™ ë¹„ë””ì˜¤ ìƒì„±"""
    print(f"ë¹„ë””ì˜¤ ìƒì„± ì¤‘: {filename}")
    
    # í™˜ê²½ ì„¤ì •
    if hasattr(agent, 'select_action'):  # DQN
        base_env = create_dqn_env(env_name)
        is_dqn = True
    else:  # DDPG
        base_env = create_ddpg_env(env_name)
        is_dqn = False
    
    # ë¹„ë””ì˜¤ ë…¹í™” í™˜ê²½ ìƒì„±
    env = gym.wrappers.RecordVideo(
        base_env, 
        video_folder=os.path.dirname(filename),
        name_prefix=os.path.basename(filename).replace('.mp4', ''),
        episode_trigger=lambda x: True  # ëª¨ë“  ì—í”¼ì†Œë“œ ë…¹í™”
    )
    
    for episode in range(episodes):
        state, _ = env.reset()
        
        while True:
            if is_dqn:
                action = agent.select_action(state, deterministic=True)
            else:
                action = agent.select_action(state, add_noise=False)
            
            state, reward, terminated, truncated, _ = env.step(action)
            
            if terminated or truncated:
                break
    
    env.close()
    print(f"ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: {filename}")


def plot_results(dqn_results: dict, ddpg_results: dict, save_path: str = 'results/training_comparison.png'):
    """í•™ìŠµ ê²°ê³¼ ì‹œê°í™”"""
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # DQN í•™ìŠµ ê³¡ì„ 
    ax1.plot(dqn_results['episode_rewards'], alpha=0.6, label='Episode Rewards')
    
    # ì´ë™ í‰ê·  ê³„ì‚°
    window = 20
    if len(dqn_results['episode_rewards']) >= window:
        moving_avg = np.convolve(dqn_results['episode_rewards'], 
                               np.ones(window)/window, mode='valid')
        ax1.plot(range(window-1, len(dqn_results['episode_rewards'])), 
                moving_avg, color='red', linewidth=2, label=f'{window}-Episode Moving Average')
    
    ax1.set_title('DQN Learning Curve')
    ax1.set_xlabel('Episode')
    ax1.set_ylabel('Reward')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # DDPG í•™ìŠµ ê³¡ì„ 
    ax2.plot(ddpg_results['episode_rewards'], alpha=0.6, label='Episode Rewards')
    
    if len(ddpg_results['episode_rewards']) >= window:
        moving_avg = np.convolve(ddpg_results['episode_rewards'], 
                               np.ones(window)/window, mode='valid')
        ax2.plot(range(window-1, len(ddpg_results['episode_rewards'])), 
                moving_avg, color='red', linewidth=2, label=f'{window}-Episode Moving Average')
    
    ax2.set_title('DDPG Learning Curve')
    ax2.set_xlabel('Episode')
    ax2.set_ylabel('Reward')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # ì—í”¼ì†Œë“œ ê¸¸ì´ ë¹„êµ
    ax3.plot(dqn_results['episode_lengths'], alpha=0.6, label='DQN')
    ax3.plot(ddpg_results['episode_lengths'], alpha=0.6, label='DDPG')
    ax3.set_title('Episode Lengths Comparison')
    ax3.set_xlabel('Episode')
    ax3.set_ylabel('Steps')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # ìµœì¢… ì„±ëŠ¥ ë°•ìŠ¤í”Œë¡¯
    recent_dqn = dqn_results['episode_rewards'][-20:] if len(dqn_results['episode_rewards']) >= 20 else dqn_results['episode_rewards']
    recent_ddpg = ddpg_results['episode_rewards'][-20:] if len(ddpg_results['episode_rewards']) >= 20 else ddpg_results['episode_rewards']
    
    ax4.boxplot([recent_dqn, recent_ddpg], labels=['DQN', 'DDPG'])
    ax4.set_title('Final Performance (Last 20 Episodes)')
    ax4.set_ylabel('Reward')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"í•™ìŠµ ê²°ê³¼ ì‹œê°í™” ì €ì¥: {save_path}")
    plt.show()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì‹œë“œ ì„¤ì •
    set_seed(42)
    
    print("ğŸš€ DQN vs DDPG ì „ì²´ ì‹¤í—˜ ì‹œì‘")
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {get_device()}")
    
    # ì„¤ì • ë¡œë“œ
    dqn_config = load_config('configs/dqn_config.yaml')
    ddpg_config = load_config('configs/ddpg_config.yaml')
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('results', exist_ok=True)
    os.makedirs('videos', exist_ok=True)
    
    # DQN í›ˆë ¨
    print("\nğŸ¯ 1ë‹¨ê³„: DQN í›ˆë ¨")
    dqn_agent, dqn_results = train_dqn(dqn_config, episodes=200)
    
    # DDPG í›ˆë ¨
    print("\nğŸ¯ 2ë‹¨ê³„: DDPG í›ˆë ¨")
    ddpg_agent, ddpg_results = train_ddpg(ddpg_config, episodes=200)
    
    # í‰ê°€
    print("\nğŸ¯ 3ë‹¨ê³„: ì—ì´ì „íŠ¸ í‰ê°€")
    dqn_eval = evaluate_agent(dqn_agent, dqn_config['environment']['name'])
    ddpg_eval = evaluate_agent(ddpg_agent, ddpg_config['environment']['name'])
    
    print(f"DQN í‰ê°€ ê²°ê³¼: {dqn_eval['mean_reward']:.2f} Â± {dqn_eval['std_reward']:.2f}")
    print(f"DDPG í‰ê°€ ê²°ê³¼: {ddpg_eval['mean_reward']:.2f} Â± {ddpg_eval['std_reward']:.2f}")
    
    # ì‹œê°í™”
    print("\nğŸ¯ 4ë‹¨ê³„: ê²°ê³¼ ì‹œê°í™”")
    plot_results(dqn_results, ddpg_results)
    
    # ë¹„ë””ì˜¤ ìƒì„±
    print("\nğŸ¯ 5ë‹¨ê³„: ë¹„ë””ì˜¤ ìƒì„±")
    try:
        create_video(dqn_agent, dqn_config['environment']['name'], 'videos/dqn_demo.mp4')
        create_video(ddpg_agent, ddpg_config['environment']['name'], 'videos/ddpg_demo.mp4')
    except Exception as e:
        print(f"ë¹„ë””ì˜¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        print("ë¹„ë””ì˜¤ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    print("\nâœ… ì „ì²´ ì‹¤í—˜ ì™„ë£Œ!")
    print("ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”:")
    print("- í•™ìŠµ ê³¡ì„ : results/training_comparison.png")
    print("- ë¹„ë””ì˜¤: videos/dqn_demo.mp4, videos/ddpg_demo.mp4")


if __name__ == "__main__":
    main()