#!/usr/bin/env python3
"""
DQN vs DDPG ì „ì²´ ë¹„êµ ì‹¤í—˜
í•™ìŠµë¶€í„° í‰ê°€ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

import os
import sys
import yaml
import torch
import numpy as np
from typing import Dict, Tuple
from tqdm import tqdm
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from src.agents import DQNAgent, DDPGAgent
from src.environments.wrappers import create_dqn_env, create_ddpg_env
from src.core.utils import set_seed
from src.core.video_manager import VideoConfig, VideoManager
from src.core.dual_recorder import DualVideoRecorder, DualRecordingConfig, create_dual_recording_env
from src.core.recording_scheduler import RecordingScheduler, create_recording_scheduler_from_config
from experiments.metrics import MetricsTracker, evaluate_agent
# ìƒˆë¡œìš´ ì‹œê°í™” ëª¨ë“ˆ import
from src.visualization.charts.learning_curves import LearningCurveVisualizer
from src.visualization.charts.comparison import ComparisonChartVisualizer
from src.visualization.charts.policy_analysis import PolicyAnalysisVisualizer
from src.visualization.core.config import VisualizationConfig
# Define create_experiment_report wrapper for backward compatibility
def create_experiment_report(results: Dict, output_dir: str):
    """Create comprehensive experiment report using new visualization system"""
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Generate learning curves
    if 'dqn' in results and 'ddpg' in results:
        visualizer = LearningCurveVisualizer()
        visualizer.plot_comparison(
            results['dqn']['metrics'],
            results['ddpg']['metrics'],
            os.path.join(output_dir, 'learning_curves.png')
        )
    
    # Generate comparison charts
    comparison_viz = ComparisonChartVisualizer()
    comparison_viz.create_comprehensive_comparison(results, output_dir)
    
    # Generate policy analysis if available
    if 'policy_analysis' in results:
        policy_viz = PolicyAnalysisVisualizer()
        policy_viz.analyze_policies(results['policy_analysis'], output_dir)


def load_config(config_path: str) -> Dict:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    if not os.path.isabs(config_path):
        config_path = os.path.join(project_root, config_path)
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def train_dqn(config: Dict, dual_recorder: DualVideoRecorder = None, 
               scheduler: RecordingScheduler = None) -> Tuple[DQNAgent, Dict]:
    """DQN í›ˆë ¨"""
    print("\n" + "="*50)
    print("DQN í›ˆë ¨ ì‹œì‘")
    print("="*50)
    
    # í™˜ê²½ ì„¤ì •
    env_name = config['environment']['name']
    state_dim = None
    action_dim = None
    
    # ì„ì‹œ í™˜ê²½ìœ¼ë¡œ ì°¨ì› í™•ì¸
    temp_env = create_dqn_env(env_name)
    state_dim = temp_env.observation_space.shape[0]
    action_dim = temp_env.action_space.n
    temp_env.close()
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = DQNAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        **config['agent']
    )
    
    # ë©”íŠ¸ë¦­ ì¶”ì 
    metrics = MetricsTracker()
    
    # í›ˆë ¨ ë£¨í”„
    for episode in tqdm(range(config['training']['episodes']), desc="DQN Training"):
        # í™˜ê²½ ì„¤ì • (ë…¹í™” ì—¬ë¶€ ê²°ì •)
        should_record = False
        if scheduler:
            should_record = scheduler.should_record_episode('dqn', episode + 1)
        
        if dual_recorder and should_record:
            env = create_dual_recording_env(env_name, dual_recorder, 'dqn', episode + 1, True)
        else:
            env = create_dqn_env(env_name)
        
        state, _ = env.reset()
        episode_reward = 0
        episode_length = 0
        
        for step in range(config['training']['max_steps_per_episode']):
            # í–‰ë™ ì„ íƒ
            action = agent.select_action(state)
            
            # í™˜ê²½ ìŠ¤í…
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # ê²½í—˜ ì €ì¥
            agent.store_transition(state, action, reward, next_state, done)
            
            # í•™ìŠµ
            if agent.buffer.is_ready(agent.batch_size):
                train_metrics = agent.update()
                metrics.add_training_step(train_metrics)
            
            state = next_state
            episode_reward += reward
            episode_length += 1
            
            if done:
                break
        
        # í™˜ê²½ ì¢…ë£Œ
        env.close()
        
        # ì—í”¼ì†Œë“œ ë©”íŠ¸ë¦­ ì¶”ê°€
        metrics.add_episode(episode_reward, episode_length)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ì— ì—í”¼ì†Œë“œ ì™„ë£Œ ë³´ê³ 
        if scheduler:
            scheduler.report_episode_completion(
                'dqn', episode + 1, episode_reward, episode_length, 
                terminated, {'training_step': episode}
            )
            if should_record:
                scheduler.mark_episode_recorded(episode + 1)
        
        # ì£¼ê¸°ì  í‰ê°€
        if episode % config['training']['eval_freq'] == 0:
            eval_env = create_dqn_env(env_name)
            eval_result = evaluate_agent(agent, eval_env, episodes=config['training']['eval_episodes'])
            eval_env.close()
            stats = metrics.get_current_stats()
            
            print(f"Episode {episode}: "
                  f"Reward {stats.get('mean_reward', 0):.2f}Â±{stats.get('std_reward', 0):.2f}, "
                  f"Eval {eval_result['mean_reward']:.2f}")
            
            if should_record:
                print(f"  ğŸ“¹ ì—í”¼ì†Œë“œ {episode + 1} ë…¹í™” ì™„ë£Œ")
    
    # ê²°ê³¼ ë°˜í™˜
    result = {
        'episode_rewards': metrics.episode_rewards,
        'episode_lengths': metrics.episode_lengths,
        'training_losses': metrics.training_losses,
        'q_values': metrics.q_values
    }
    
    return agent, result


def train_ddpg(config: Dict, dual_recorder: DualVideoRecorder = None,
                scheduler: RecordingScheduler = None) -> Tuple[DDPGAgent, Dict]:
    """DDPG í›ˆë ¨"""
    print("\n" + "="*50)
    print("DDPG í›ˆë ¨ ì‹œì‘")
    print("="*50)
    
    # í™˜ê²½ ì„¤ì •
    env_name = config['environment']['name']
    
    # ì„ì‹œ í™˜ê²½ìœ¼ë¡œ ì°¨ì› í™•ì¸
    temp_env = create_ddpg_env(env_name)
    state_dim = temp_env.observation_space.shape[0]
    action_dim = temp_env.action_space.shape[0]
    temp_env.close()
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = DDPGAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        **config['agent']
    )
    
    # ë©”íŠ¸ë¦­ ì¶”ì 
    metrics = MetricsTracker()
    
    # ì›Œë°ì—… (ë¬´ì‘ìœ„ í–‰ë™)
    print("ì›Œë°ì—… ì¤‘...")
    warmup_env = create_ddpg_env(env_name)
    for _ in range(config['training'].get('warmup_steps', 1000)):
        state, _ = warmup_env.reset()
        action = warmup_env.action_space.sample()
        next_state, reward, terminated, truncated, _ = warmup_env.step(action)
        done = terminated or truncated
        agent.store_transition(state, action, reward, next_state, done)
        
        if done:
            continue
    warmup_env.close()
    
    # í›ˆë ¨ ë£¨í”„
    for episode in tqdm(range(config['training']['episodes']), desc="DDPG Training"):
        # í™˜ê²½ ì„¤ì • (ë…¹í™” ì—¬ë¶€ ê²°ì •)
        should_record = False
        if scheduler:
            should_record = scheduler.should_record_episode('ddpg', episode + 1)
        
        if dual_recorder and should_record:
            env = create_dual_recording_env(env_name, dual_recorder, 'ddpg', episode + 1, True)
        else:
            env = create_ddpg_env(env_name)
        
        state, _ = env.reset()
        episode_reward = 0
        episode_length = 0
        agent.reset_noise()
        
        for step in range(config['training']['max_steps_per_episode']):
            # í–‰ë™ ì„ íƒ (ë…¸ì´ì¦ˆ í¬í•¨)
            action = agent.select_action(state, add_noise=True)
            
            # í™˜ê²½ ìŠ¤í…
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # ê²½í—˜ ì €ì¥
            agent.store_transition(state, action, reward, next_state, done)
            
            # í•™ìŠµ
            if agent.buffer.is_ready(agent.batch_size):
                train_metrics = agent.update()
                metrics.add_training_step(train_metrics)
            
            state = next_state
            episode_reward += reward
            episode_length += 1
            
            if done:
                break
        
        # í™˜ê²½ ì¢…ë£Œ
        env.close()
        
        # ì—í”¼ì†Œë“œ ë©”íŠ¸ë¦­ ì¶”ê°€
        metrics.add_episode(episode_reward, episode_length)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ì— ì—í”¼ì†Œë“œ ì™„ë£Œ ë³´ê³ 
        if scheduler:
            scheduler.report_episode_completion(
                'ddpg', episode + 1, episode_reward, episode_length,
                terminated, {'training_step': episode}
            )
            if should_record:
                scheduler.mark_episode_recorded(episode + 1)
        
        # ì£¼ê¸°ì  í‰ê°€
        if episode % config['training']['eval_freq'] == 0:
            eval_env = create_ddpg_env(env_name)
            eval_result = evaluate_agent(agent, eval_env, episodes=config['training']['eval_episodes'])
            eval_env.close()
            stats = metrics.get_current_stats()
            
            print(f"Episode {episode}: "
                  f"Reward {stats.get('mean_reward', 0):.2f}Â±{stats.get('std_reward', 0):.2f}, "
                  f"Eval {eval_result['mean_reward']:.2f}")
            
            if should_record:
                print(f"  ğŸ“¹ ì—í”¼ì†Œë“œ {episode + 1} ë…¹í™” ì™„ë£Œ")
    
    # ê²°ê³¼ ë°˜í™˜
    result = {
        'episode_rewards': metrics.episode_rewards,
        'episode_lengths': metrics.episode_lengths,
        'training_losses': metrics.training_losses,
        'q_values': metrics.q_values
    }
    
    return agent, result


def run_final_evaluation(dqn_agent: DQNAgent, ddpg_agent: DDPGAgent) -> Tuple[Dict, Dict]:
    """ìµœì¢… í‰ê°€"""
    print("\n" + "="*50)
    print("ìµœì¢… í‰ê°€")
    print("="*50)
    
    # DQN í‰ê°€
    dqn_env = create_dqn_env()
    dqn_eval = evaluate_agent(dqn_agent, dqn_env, episodes=50, deterministic=True)
    dqn_env.close()
    
    print(f"DQN ìµœì¢… ì„±ëŠ¥: {dqn_eval['mean_reward']:.2f}Â±{dqn_eval['std_reward']:.2f}")
    
    # DDPG í‰ê°€
    ddpg_env = create_ddpg_env()
    ddpg_eval = evaluate_agent(ddpg_agent, ddpg_env, episodes=50, deterministic=True)
    ddpg_env.close()
    
    print(f"DDPG ìµœì¢… ì„±ëŠ¥: {ddpg_eval['mean_reward']:.2f}Â±{ddpg_eval['std_reward']:.2f}")
    
    return dqn_eval, ddpg_eval


def main():
    """ë©”ì¸ ì‹¤í—˜ ì‹¤í–‰"""
    parser = argparse.ArgumentParser(description='DQN vs DDPG ë¹„êµ ì‹¤í—˜')
    parser.add_argument('--seed', type=int, default=42, help='ëœë¤ ì‹œë“œ')
    parser.add_argument('--save-models', action='store_true', help='í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥')
    parser.add_argument('--results-dir', type=str, default='results', help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    
    # ì´ì¤‘ ë…¹í™” ì˜µì…˜
    parser.add_argument('--dual-video', action='store_true', help='ì´ì¤‘ ë¹„ë””ì˜¤ ë…¹í™” í™œì„±í™”')
    parser.add_argument('--video-config', type=str, default='configs/video_recording.yaml', 
                       help='ë¹„ë””ì˜¤ ë…¹í™” ì„¤ì • íŒŒì¼')
    parser.add_argument('--video-preset', type=str, choices=['low', 'medium', 'high', 'demo'],
                       help='ë¹„ë””ì˜¤ í’ˆì§ˆ í”„ë¦¬ì…‹')
    
    args = parser.parse_args()
    
    # ì‹œë“œ ì„¤ì •
    set_seed(args.seed)
    
    # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ë£¨íŠ¸ë¡œ ë³€ê²½
    os.chdir(project_root)
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.results_dir, exist_ok=True)
    
    print("DQN vs DDPG ë¹„êµ ì‹¤í—˜ ì‹œì‘")
    print(f"ì‹œë“œ: {args.seed}")
    print(f"ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {args.results_dir}")
    print(f"ì´ì¤‘ ë¹„ë””ì˜¤ ë…¹í™”: {'í™œì„±í™”' if args.dual_video else 'ë¹„í™œì„±í™”'}")
    
    # ì´ì¤‘ ë…¹í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    dual_recorder = None
    scheduler = None
    
    if args.dual_video:
        print("\nğŸ¬ ì´ì¤‘ ë…¹í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # ë¹„ë””ì˜¤ ì„¤ì • ë¡œë“œ
        try:
            video_config_dict = load_config(args.video_config)
        except FileNotFoundError:
            print(f"[WARNING] ë¹„ë””ì˜¤ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.video_config}")
            print("[INFO] ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            video_config_dict = {}
        
        # ë¹„ë””ì˜¤ ë§¤ë‹ˆì € ì„¤ì •
        if args.video_preset:
            video_config = VideoConfig.get_preset(args.video_preset)
        else:
            video_config = VideoConfig.from_yaml(args.video_config) if video_config_dict else VideoConfig()
        
        video_manager = VideoManager(video_config)
        
        # ì´ì¤‘ ë…¹í™” ì„¤ì •
        dual_config = DualRecordingConfig.from_yaml_config(video_config_dict)
        dual_recorder = DualVideoRecorder(video_manager, dual_config)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        try:
            scheduler = create_recording_scheduler_from_config(args.video_config)
        except:
            print("[INFO] ê¸°ë³¸ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            from src.core.recording_scheduler import create_default_recording_scheduler
            scheduler = create_default_recording_scheduler()
        
        print("âœ… ì´ì¤‘ ë…¹í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        summary = scheduler.get_recording_summary()
        print(f"ğŸ“‹ ë…¹í™” ê³„íš: ì´ˆê¸° {len(scheduler.config.initial_episodes)}ê°œ, "
              f"ì£¼ê¸° {scheduler.config.interval_episodes}ì—í”¼ì†Œë“œë§ˆë‹¤, "
              f"ìµœëŒ€ {summary['config']['max_recordings']}ê°œ ì„ íƒì  ë…¹í™”")
    
    # ì„¤ì • ë¡œë“œ
    dqn_config = load_config('configs/dqn_config.yaml')
    ddpg_config = load_config('configs/ddpg_config.yaml')
    
    # ì‹œë“œ ì„¤ì •ì„ configì— ë°˜ì˜
    dqn_config['environment']['seed'] = args.seed
    ddpg_config['environment']['seed'] = args.seed
    
    # DQN í›ˆë ¨
    dqn_agent, dqn_results = train_dqn(dqn_config, dual_recorder, scheduler)
    
    # DDPG í›ˆë ¨
    ddpg_agent, ddpg_results = train_ddpg(ddpg_config, dual_recorder, scheduler)
    
    # ìµœì¢… í‰ê°€
    dqn_eval, ddpg_eval = run_final_evaluation(dqn_agent, ddpg_agent)
    
    # ê²°ê³¼ ì‹œê°í™” (ìƒˆë¡œìš´ ëª¨ë“ˆ ì‚¬ìš©)
    print("\nê²°ê³¼ ì‹œê°í™” ì¤‘...")
    
    # ì‹œê°í™” ì„¤ì •
    viz_config = VisualizationConfig()
    
    # 1. í•™ìŠµ ê³¡ì„  ì‹œê°í™”
    with LearningCurveVisualizer(output_dir=args.results_dir, config=viz_config) as viz:
        learning_data = {
            'dqn': dqn_results,
            'ddpg': ddpg_results
        }
        learning_curves_path = viz.plot_comprehensive_learning_curves(
            dqn_results, ddpg_results,
            save_filename="learning_curves.png"
        )
        print(f"  âœ… í•™ìŠµ ê³¡ì„  ì €ì¥: {learning_curves_path}")
    
    # 2. ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”
    with ComparisonChartVisualizer(output_dir=args.results_dir, config=viz_config) as viz:
        comparison_data = {
            'dqn': {'episode_rewards': dqn_results['episode_rewards'], **dqn_eval},
            'ddpg': {'episode_rewards': ddpg_results['episode_rewards'], **ddpg_eval}
        }
        comparison_path = viz.plot_performance_comparison(
            comparison_data['dqn'], comparison_data['ddpg'],
            save_filename="performance_comparison.png"
        )
        print(f"  âœ… ì„±ëŠ¥ ë¹„êµ ì €ì¥: {comparison_path}")
    
    # 3. ê²°ì •ì  ì •ì±… ì‹œê°í™” 
    with PolicyAnalysisVisualizer(output_dir=args.results_dir, config=viz_config) as viz:
        dqn_env = create_dqn_env()
        ddpg_env = create_ddpg_env()
        
        policy_path = viz.visualize_deterministic_policies(
            dqn_agent, ddpg_agent, dqn_env, ddpg_env,
            save_filename="deterministic_policy_analysis.png"
        )
        print(f"  âœ… ì •ì±… ë¶„ì„ ì €ì¥: {policy_path}")
        
        dqn_env.close()
        ddpg_env.close()
    
    # 4. ì‹¤í—˜ ë¦¬í¬íŠ¸ ìƒì„±
    experiment_results = {
        'dqn_eval': dqn_eval,
        'ddpg_eval': ddpg_eval,
        'dqn_training': dqn_results,
        'ddpg_training': ddpg_results
    }
    
    create_experiment_report(
        experiment_results,
        save_path=f"{args.results_dir}/experiment_report.md"
    )
    
    # ëª¨ë¸ ì €ì¥
    if args.save_models:
        print("í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥ ì¤‘...")
        dqn_agent.save(f"{args.results_dir}/dqn_model.pth")
        ddpg_agent.save(f"{args.results_dir}/ddpg_model.pth")
    
    # ì´ì¤‘ ë…¹í™” í†µê³„ ì¶œë ¥
    if args.dual_video and dual_recorder and scheduler:
        print("\nğŸ¬ ë¹„ë””ì˜¤ ë…¹í™” í†µê³„:")
        recording_stats = dual_recorder.get_recording_stats()
        print(f"  ğŸ“Š ì´ ë…¹í™” ì—í”¼ì†Œë“œ: {recording_stats['total_episodes']}")
        print(f"  ğŸ“¹ ì „ì²´ ë…¹í™”: {recording_stats['full_recordings']}")
        print(f"  â­ ì„ íƒì  ë…¹í™”: {recording_stats['selective_recordings']}")
        print(f"  ğŸï¸ ì²˜ë¦¬ëœ í”„ë ˆì„: {recording_stats['total_frames_processed']:,}")
        print(f"  â±ï¸ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {recording_stats['average_processing_time']*1000:.2f}ms/í”„ë ˆì„")
        
        # ì•Œê³ ë¦¬ì¦˜ë³„ ì„±ëŠ¥ ìš”ì•½
        for algorithm in ['dqn', 'ddpg']:
            summary = scheduler.get_recording_summary(algorithm)
            if 'performance' in summary:
                perf = summary['performance']
                print(f"\n  ğŸ“ˆ {algorithm.upper()} ì„±ëŠ¥:")
                print(f"    â€¢ ìµœê³  ì ìˆ˜: {perf['best_score']:.2f}")
                print(f"    â€¢ í‰ê·  ì ìˆ˜: {perf['average_score']:.2f}")
                print(f"    â€¢ ë‹¬ì„± ë§ˆì¼ìŠ¤í†¤: {perf['achieved_milestones']}")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ì €ì¥
        schedule_save_path = f"{args.results_dir}/recording_schedule.json"
        scheduler.save_schedule_state(schedule_save_path)
        
        # ë¹„ë””ì˜¤ ì €ì¥ ê²½ë¡œ ì•ˆë‚´
        print(f"\nğŸ“ ë¹„ë””ì˜¤ ì €ì¥ ìœ„ì¹˜:")
        print(f"  â€¢ ì „ì²´ ë…¹í™”: videos/{{algorithm}}/full/")
        print(f"  â€¢ ì„ íƒì  ë…¹í™”: videos/{{algorithm}}/highlights/")
        print(f"  â€¢ ì„¤ì • ë° í†µê³„: {schedule_save_path}")
    
    print("\n" + "="*50)
    print("ì‹¤í—˜ ì™„ë£Œ!")
    print("="*50)
    print(f"ê²°ê³¼ëŠ” '{args.results_dir}' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("\nì£¼ìš” ê²°ê³¼:")
    print(f"- DQN: {dqn_eval['mean_reward']:.2f}Â±{dqn_eval['std_reward']:.2f}")
    print(f"- DDPG: {ddpg_eval['mean_reward']:.2f}Â±{ddpg_eval['std_reward']:.2f}")
    
    print("\nê²°ì •ì  ì •ì±… í™•ì¸:")
    print("âœ“ DQN: Q-ê°’ argmaxë¥¼ í†µí•œ ì•”ë¬µì  ê²°ì •ì  ì •ì±…")
    print("âœ“ DDPG: ì•¡í„° ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•œ ëª…ì‹œì  ê²°ì •ì  ì •ì±…")


if __name__ == "__main__":
    main()