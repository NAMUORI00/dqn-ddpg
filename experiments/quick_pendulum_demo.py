#!/usr/bin/env python3
"""
Pendulum-v1 í™˜ê²½ì—ì„œ DDPG vs DQN ë¹ ë¥¸ ë°ëª¨

ì‹œê°„ì„ ë‹¨ì¶•í•˜ì—¬ DDPG ìš°ìœ„ë¥¼ ë¹ ë¥´ê²Œ í™•ì¸í•˜ëŠ” ë°ëª¨ì…ë‹ˆë‹¤.
"""

import os
import sys
import json
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from typing import Dict, List, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

from src.agents.ddpg_agent import DDPGAgent
from src.agents.dqn_agent import DQNAgent  
from src.environments.wrappers import create_ddpg_env
from src.core.utils import set_seed
import gymnasium as gym


class PendulumDQNWrapper(gym.Wrapper):
    """Pendulum í™˜ê²½ì„ DQNìš©ìœ¼ë¡œ ì´ì‚°í™”í•˜ëŠ” ë˜í¼"""
    
    def __init__(self, env: gym.Env, num_actions: int = 11):  # ë” ì ì€ í–‰ë™ìœ¼ë¡œ ë¹ ë¥¸ í•™ìŠµ
        super().__init__(env)
        self.num_actions = num_actions
        
        # ì—°ì† í–‰ë™ [-2, 2]ë¥¼ ì´ì‚° í–‰ë™ìœ¼ë¡œ ë³€í™˜
        self.action_space = gym.spaces.Discrete(num_actions)
        self.action_mapping = np.linspace(-2.0, 2.0, num_actions)
        
    def step(self, action: int) -> Tuple:
        # ì´ì‚° í–‰ë™ì„ ì—°ì† í–‰ë™ìœ¼ë¡œ ë³€í™˜
        continuous_action = np.array([self.action_mapping[action]], dtype=np.float32)
        return self.env.step(continuous_action)


def create_pendulum_dqn_env() -> gym.Env:
    """DQNìš© Pendulum í™˜ê²½ ìƒì„±"""
    env = gym.make("Pendulum-v1")
    env = PendulumDQNWrapper(env, num_actions=11)
    return env


def quick_test_agent(agent, env, episodes: int = 20) -> Dict:
    """ì—ì´ì „íŠ¸ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸"""
    scores = []
    
    print(f"Testing {agent.__class__.__name__} for {episodes} episodes...")
    
    for episode in range(episodes):
        state, _ = env.reset()
        total_reward = 0
        step_count = 0
        max_steps = 200
        
        while step_count < max_steps:
            # í–‰ë™ ì„ íƒ (DQNì€ epsilon-greedy, DDPGëŠ” ë…¸ì´ì¦ˆ ì¶”ê°€)
            if hasattr(agent, 'select_action'):
                if 'DDPG' in agent.__class__.__name__:
                    action = agent.select_action(state, add_noise=True)
                else:  # DQN
                    action = agent.select_action(state)
            
            next_state, reward, terminated, truncated, _ = env.step(action)
            
            # ê²½í—˜ ì €ì¥ ë° í•™ìŠµ
            if hasattr(agent, 'store_transition'):
                agent.store_transition(state, action, reward, next_state, terminated or truncated)
                agent.update()
            
            state = next_state
            total_reward += reward
            step_count += 1
            
            if terminated or truncated:
                break
        
        scores.append(total_reward)
        
        if (episode + 1) % 5 == 0:
            recent_avg = np.mean(scores[-5:])
            print(f"Episode {episode + 1}: Recent avg = {recent_avg:.2f}")
    
    return {
        'scores': scores,
        'final_score': np.mean(scores[-10:]) if len(scores) >= 10 else np.mean(scores),
        'best_score': max(scores),
        'worst_score': min(scores),
        'std_score': np.std(scores)
    }


def evaluate_final_performance(agent, env, episodes: int = 10) -> float:
    """ìµœì¢… ì„±ëŠ¥ í‰ê°€ (ë…¸ì´ì¦ˆ ì—†ìŒ)"""
    total_rewards = []
    
    for _ in range(episodes):
        state, _ = env.reset()
        total_reward = 0
        step_count = 0
        max_steps = 200
        
        while step_count < max_steps:
            # í‰ê°€ ì‹œì—ëŠ” ë…¸ì´ì¦ˆ ì—†ìŒ
            if hasattr(agent, 'select_action'):
                if 'DDPG' in agent.__class__.__name__:
                    action = agent.select_action(state, add_noise=False)
                else:  # DQN
                    action = agent.select_action(state)
            
            next_state, reward, terminated, truncated, _ = env.step(action)
            
            state = next_state
            total_reward += reward
            step_count += 1
            
            if terminated or truncated:
                break
        
        total_rewards.append(total_reward)
    
    return np.mean(total_rewards)


def run_quick_pendulum_demo():
    """ë¹ ë¥¸ Pendulum ë°ëª¨ ì‹¤í–‰"""
    print("ğŸ¯ Pendulum-v1 ë¹ ë¥¸ ë°ëª¨: DDPG vs DQN")
    print("=" * 50)
    
    # ì‹œë“œ ì„¤ì •
    seed = 42
    set_seed(seed)
    
    # í™˜ê²½ ì„¤ì •
    ddpg_env = create_ddpg_env("Pendulum-v1")
    dqn_env = create_pendulum_dqn_env()
    
    # ì—ì´ì „íŠ¸ ì„¤ì • (ë” ê°„ë‹¨í•œ ì„¤ì •)
    state_dim = 3
    action_dim = 1
    
    ddpg_agent = DDPGAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        actor_lr=0.001,  # ë” ë¹ ë¥¸ í•™ìŠµ
        critic_lr=0.002,
        gamma=0.99,
        tau=0.01,  # ë” ë¹ ë¥¸ íƒ€ê²Ÿ ì—…ë°ì´íŠ¸
        buffer_size=10000,  # ë” ì‘ì€ ë²„í¼
        batch_size=32,
        noise_sigma=0.2
    )
    
    dqn_agent = DQNAgent(
        state_dim=state_dim,
        action_dim=11,
        learning_rate=0.001,
        epsilon=1.0,
        epsilon_min=0.1,
        epsilon_decay=0.99,  # ë” ë¹ ë¥¸ íƒí—˜ ê°ì†Œ
        gamma=0.99,
        buffer_size=10000,
        batch_size=32,
        target_update_freq=50  # ë” ìì£¼ ì—…ë°ì´íŠ¸
    )
    
    # ë¹ ë¥¸ í•™ìŠµ í…ŒìŠ¤íŠ¸
    training_episodes = 50
    
    print("ğŸ¤– DDPG ë¹ ë¥¸ í•™ìŠµ...")
    ddpg_results = quick_test_agent(ddpg_agent, ddpg_env, training_episodes)
    
    print("\nğŸ¤– DQN ë¹ ë¥¸ í•™ìŠµ...")
    dqn_results = quick_test_agent(dqn_agent, dqn_env, training_episodes)
    
    # ìµœì¢… ì„±ëŠ¥ í‰ê°€
    print("\nğŸ“Š ìµœì¢… ì„±ëŠ¥ í‰ê°€...")
    ddpg_final = evaluate_final_performance(ddpg_agent, ddpg_env)
    dqn_final = evaluate_final_performance(dqn_agent, dqn_env)
    
    # ê²°ê³¼ ì •ë¦¬
    results = {
        'experiment_config': {
            'environment': 'Pendulum-v1',
            'training_episodes': training_episodes,
            'evaluation_episodes': 10,
            'seed': seed,
            'note': 'Quick demo for DDPG advantage demonstration'
        },
        'training_results': {
            'ddpg': ddpg_results,
            'dqn': dqn_results
        },
        'final_evaluation': {
            'ddpg_final': ddpg_final,
            'dqn_final': dqn_final,
            'performance_ratio': abs(ddpg_final / dqn_final) if dqn_final != 0 else float('inf'),
            'ddpg_advantage': bool(ddpg_final > dqn_final)
        },
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 50)
    print("ğŸ“Š ë¹ ë¥¸ ë°ëª¨ ê²°ê³¼")
    print("=" * 50)
    print(f"DDPG í•™ìŠµ ì„±ëŠ¥: {ddpg_results['final_score']:.2f}")
    print(f"DQN í•™ìŠµ ì„±ëŠ¥:  {dqn_results['final_score']:.2f}")
    print(f"DDPG ìµœì¢… í‰ê°€: {ddpg_final:.2f}")
    print(f"DQN ìµœì¢… í‰ê°€:  {dqn_final:.2f}")
    print(f"ì„±ëŠ¥ ë¹„ìœ¨:      {results['final_evaluation']['performance_ratio']:.2f}x")
    print(f"DDPG ìš°ìœ„:      {'âœ…' if results['final_evaluation']['ddpg_advantage'] else 'âŒ'}")
    
    # ê²°ê³¼ ì €ì¥
    os.makedirs('results/pendulum_comparison', exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    result_file = f'results/pendulum_comparison/quick_demo_{timestamp}.json'
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {result_file}")
    
    # ê°„ë‹¨í•œ ì‹œê°í™”
    create_quick_visualization(results, timestamp)
    
    return results


def create_quick_visualization(results: Dict, timestamp: str):
    """ë¹ ë¥¸ ì‹œê°í™” ìƒì„±"""
    ddpg_scores = results['training_results']['ddpg']['scores']
    dqn_scores = results['training_results']['dqn']['scores']
    
    plt.figure(figsize=(12, 6))
    
    # 1. í•™ìŠµ ê³¡ì„ 
    plt.subplot(1, 2, 1)
    episodes = range(1, len(ddpg_scores) + 1)
    plt.plot(episodes, ddpg_scores, label='DDPG', color='blue', alpha=0.7)
    plt.plot(episodes, dqn_scores, label='DQN', color='red', alpha=0.7)
    
    # ì´ë™ í‰ê· 
    window = 10
    if len(ddpg_scores) >= window:
        ddpg_ma = np.convolve(ddpg_scores, np.ones(window)/window, mode='valid')
        dqn_ma = np.convolve(dqn_scores, np.ones(window)/window, mode='valid')
        ma_episodes = range(window, len(ddpg_scores) + 1)
        plt.plot(ma_episodes, ddpg_ma, label='DDPG (MA10)', color='darkblue', linewidth=2)
        plt.plot(ma_episodes, dqn_ma, label='DQN (MA10)', color='darkred', linewidth=2)
    
    plt.title('Pendulum-v1: Quick Learning Demo')
    plt.xlabel('Episodes')
    plt.ylabel('Reward')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 2. ìµœì¢… ì„±ëŠ¥ ë¹„êµ
    plt.subplot(1, 2, 2)
    algorithms = ['DDPG', 'DQN']
    final_scores = [
        results['final_evaluation']['ddpg_final'],
        results['final_evaluation']['dqn_final']
    ]
    colors = ['blue', 'red']
    
    bars = plt.bar(algorithms, final_scores, color=colors, alpha=0.7)
    plt.title('Final Performance (No Noise Evaluation)')
    plt.ylabel('Average Reward')
    
    # ê°’ í‘œì‹œ
    for bar, score in zip(bars, final_scores):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,
                f'{score:.1f}', ha='center', va='bottom', fontweight='bold')
    
    plt.grid(True, alpha=0.3, axis='y')
    
    # ì„±ëŠ¥ ë¹„ìœ¨ ì¶”ê°€ í‘œì‹œ
    ratio = results['final_evaluation']['performance_ratio']
    plt.text(0.5, max(final_scores) * 0.8, f'Ratio: {ratio:.1f}x', 
             ha='center', fontsize=12, fontweight='bold',
             bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
    
    plt.tight_layout()
    
    # ì €ì¥
    viz_file = f'results/pendulum_comparison/quick_demo_viz_{timestamp}.png'
    plt.savefig(viz_file, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š ì‹œê°í™” ì €ì¥: {viz_file}")
    
    plt.show()


if __name__ == "__main__":
    results = run_quick_pendulum_demo()