#!/usr/bin/env python3
"""
ê²°ì •ì  ì •ì±… ì‹¬ì¸µ ë¶„ì„ ë„êµ¬

DQNê³¼ DDPGì˜ ê²°ì •ì  ì •ì±… íŠ¹ì„±ì„ ë‹¤ê°ë„ë¡œ ë¶„ì„í•©ë‹ˆë‹¤:
- DQN: Q-ê°’ ê¸°ë°˜ ì•”ë¬µì  ê²°ì •ì„± (argmax ì¼ê´€ì„±)
- DDPG: ì•¡í„° ì¶œë ¥ ëª…ì‹œì  ê²°ì •ì„± (ì¶œë ¥ ì¼ê´€ì„±)
"""

import os
import sys
import json
import numpy as np
import torch
import matplotlib.pyplot as plt
import seaborn as sns

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False
from typing import Dict, List, Tuple, Optional
import argparse
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from src.agents import DQNAgent, DDPGAgent
from src.environments.wrappers import create_dqn_env, create_ddpg_env
from src.core.utils import set_seed


class DeterministicPolicyAnalyzer:
    """ê²°ì •ì  ì •ì±… ë¶„ì„ê¸°"""
    
    def __init__(self, results_dir: str = "results"):
        self.results_dir = results_dir
        self.analysis_results = {}
        
    def analyze_dqn_determinism(self, agent: DQNAgent, env, num_tests: int = 100) -> Dict:
        """DQNì˜ ê²°ì •ì  ì •ì±… íŠ¹ì„± ë¶„ì„"""
        print("DQN ê²°ì •ì  ì •ì±… ë¶„ì„ ì¤‘...")
        
        results = {
            'q_value_consistency': [],
            'action_consistency': [],
            'q_value_differences': [],
            'argmax_confidence': [],
            'state_action_mapping': {},
            'epsilon_effect': {}
        }
        
        # í…ŒìŠ¤íŠ¸ ìƒíƒœë“¤ ìƒì„±
        test_states = []
        for _ in range(num_tests):
            state = env.observation_space.sample()
            test_states.append(state)
        
        # 1. Q-ê°’ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸ (ë™ì¼ ìƒíƒœì— ëŒ€í•œ ë°˜ë³µ Q-ê°’ ê³„ì‚°)
        print("  - Q-ê°’ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸")
        for i, state in enumerate(tqdm(test_states[:20], desc="Q-ê°’ ì¼ê´€ì„±")):
            q_values_repeated = []
            actions_repeated = []
            
            # ë™ì¼ ìƒíƒœì— ëŒ€í•´ 10ë²ˆ ë°˜ë³µ ê³„ì‚°
            for _ in range(10):
                q_values = agent.get_q_values(state)
                if isinstance(q_values, torch.Tensor):
                    q_values_np = q_values.detach().cpu().numpy()
                    action = torch.argmax(q_values).item()
                else:
                    q_values_np = q_values
                    action = np.argmax(q_values)
                
                q_values_repeated.append(q_values_np)
                actions_repeated.append(action)
            
            # Q-ê°’ ë¶„ì‚° ê³„ì‚° (ì¼ê´€ì„± ì¸¡ì •)
            q_values_array = np.array(q_values_repeated)
            q_value_std = np.std(q_values_array, axis=0)
            
            # í–‰ë™ ì¼ê´€ì„± ê³„ì‚° (ëª¨ë“  ë°˜ë³µì—ì„œ ê°™ì€ í–‰ë™ì„ ì„ íƒí–ˆëŠ”ê°€?)
            action_consistency = len(set(actions_repeated)) == 1
            
            results['q_value_consistency'].append(q_value_std.mean())
            results['action_consistency'].append(action_consistency)
            
            # ìƒíƒœ-í–‰ë™ ë§¤í•‘ ì €ì¥
            results['state_action_mapping'][f'state_{i}'] = {
                'q_values': q_values_repeated[0].tolist(),
                'selected_action': int(actions_repeated[0]),
                'action_consistency': action_consistency
            }
        
        # 2. Q-ê°’ ì°¨ì´ ë¶„ì„ (ìµœê³  Q-ê°’ê³¼ ë‘ ë²ˆì§¸ Q-ê°’ì˜ ì°¨ì´)
        print("  - Q-ê°’ ì°¨ì´ ë¶„ì„")
        for state in tqdm(test_states, desc="Q-ê°’ ì°¨ì´"):
            q_values = agent.get_q_values(state)
            if isinstance(q_values, torch.Tensor):
                q_values = q_values.detach().cpu().numpy()
            sorted_q = np.sort(q_values)[::-1]
            
            if len(sorted_q) > 1:
                q_diff = sorted_q[0] - sorted_q[1]
                results['q_value_differences'].append(q_diff)
                
                # argmax ì‹ ë¢°ë„ (ì°¨ì´ê°€ í´ìˆ˜ë¡ ê²°ì •ì´ í™•ì‹¤í•¨)
                confidence = q_diff / (np.abs(sorted_q[0]) + 1e-8)
                results['argmax_confidence'].append(confidence)
        
        # 3. ì—¡ì‹¤ë¡  íš¨ê³¼ ë¶„ì„
        print("  - ì—¡ì‹¤ë¡  íš¨ê³¼ ë¶„ì„")
        original_epsilon = agent.epsilon
        test_state = test_states[0]
        
        epsilon_values = [0.0, 0.1, 0.5, 1.0]
        for eps in epsilon_values:
            agent.epsilon = eps
            actions = []
            for _ in range(100):
                action = agent.select_action(test_state)
                actions.append(action)
            
            # í–‰ë™ ë‹¤ì–‘ì„± ì¸¡ì •
            unique_actions = len(set(actions))
            action_distribution = np.bincount(actions, minlength=env.action_space.n)
            
            results['epsilon_effect'][f'eps_{eps}'] = {
                'unique_actions': unique_actions,
                'action_distribution': action_distribution.tolist(),
                'entropy': -np.sum(action_distribution / 100 * np.log(action_distribution / 100 + 1e-8))
            }
        
        agent.epsilon = original_epsilon
        
        # í†µê³„ ê³„ì‚°
        results['statistics'] = {
            'mean_q_consistency': np.mean(results['q_value_consistency']),
            'action_consistency_rate': np.mean(results['action_consistency']),
            'mean_q_difference': np.mean(results['q_value_differences']),
            'mean_argmax_confidence': np.mean(results['argmax_confidence'])
        }
        
        return results
    
    def analyze_ddpg_determinism(self, agent: DDPGAgent, env, num_tests: int = 100) -> Dict:
        """DDPGì˜ ê²°ì •ì  ì •ì±… íŠ¹ì„± ë¶„ì„"""
        print("DDPG ê²°ì •ì  ì •ì±… ë¶„ì„ ì¤‘...")
        
        results = {
            'action_consistency': [],
            'action_variance': [],
            'noise_effect': {},
            'state_action_mapping': {},
            'deterministic_vs_noisy': []
        }
        
        # í…ŒìŠ¤íŠ¸ ìƒíƒœë“¤ ìƒì„±
        test_states = []
        for _ in range(num_tests):
            state = env.observation_space.sample()
            test_states.append(state)
        
        # 1. ì•¡í„° ì¶œë ¥ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
        print("  - ì•¡í„° ì¶œë ¥ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸")
        for i, state in enumerate(tqdm(test_states[:20], desc="ì•¡í„° ì¼ê´€ì„±")):
            actions_repeated = []
            
            # ë™ì¼ ìƒíƒœì— ëŒ€í•´ 10ë²ˆ ë°˜ë³µ (ë…¸ì´ì¦ˆ ì—†ì´)
            for _ in range(10):
                action = agent.get_deterministic_action(state)
                actions_repeated.append(action)
            
            actions_array = np.array(actions_repeated)
            
            # ì•¡ì…˜ ë¶„ì‚° ê³„ì‚° (ë‚®ì„ìˆ˜ë¡ ê²°ì •ì )
            action_variance = np.var(actions_array, axis=0)
            action_std = np.std(actions_array, axis=0)
            
            # ì™„ì „í•œ ì¼ê´€ì„±ì¸ì§€ í™•ì¸ (ë¶„ì‚°ì´ 0ì— ê°€ê¹Œìš´ê°€?)
            is_consistent = np.allclose(action_variance, 0, atol=1e-6)
            
            results['action_consistency'].append(is_consistent)
            results['action_variance'].append(action_variance.mean())
            
            # ìƒíƒœ-í–‰ë™ ë§¤í•‘ ì €ì¥
            results['state_action_mapping'][f'state_{i}'] = {
                'deterministic_action': actions_repeated[0].tolist(),
                'action_std': action_std.tolist(),
                'is_consistent': is_consistent
            }
        
        # 2. ë…¸ì´ì¦ˆ íš¨ê³¼ ë¶„ì„
        print("  - ë…¸ì´ì¦ˆ íš¨ê³¼ ë¶„ì„")
        test_state = test_states[0]
        
        # ë…¸ì´ì¦ˆ ì—†ëŠ” í–‰ë™ vs ë…¸ì´ì¦ˆ ìˆëŠ” í–‰ë™
        deterministic_actions = []
        noisy_actions = []
        
        for _ in range(50):
            det_action = agent.select_action(test_state, add_noise=False)
            noisy_action = agent.select_action(test_state, add_noise=True)
            
            deterministic_actions.append(det_action)
            noisy_actions.append(noisy_action)
        
        det_actions_array = np.array(deterministic_actions)
        noisy_actions_array = np.array(noisy_actions)
        
        # ë…¸ì´ì¦ˆë¡œ ì¸í•œ ë³€í™”ëŸ‰ ì¸¡ì •
        action_differences = []
        for det, noisy in zip(deterministic_actions, noisy_actions):
            diff = np.linalg.norm(np.array(noisy) - np.array(det))
            action_differences.append(diff)
        
        results['noise_effect'] = {
            'deterministic_std': np.std(det_actions_array, axis=0).tolist(),
            'noisy_std': np.std(noisy_actions_array, axis=0).tolist(),
            'mean_noise_impact': np.mean(action_differences),
            'noise_impact_std': np.std(action_differences)
        }
        
        results['deterministic_vs_noisy'] = action_differences
        
        # 3. ë‹¤ì–‘í•œ ë…¸ì´ì¦ˆ ê°•ë„ í…ŒìŠ¤íŠ¸
        print("  - ë…¸ì´ì¦ˆ ê°•ë„ ì˜í–¥ ë¶„ì„")
        original_noise_std = agent.noise.sigma if hasattr(agent.noise, 'sigma') else 0.1
        
        noise_levels = [0.0, 0.05, 0.1, 0.2, 0.5]
        for noise_std in noise_levels:
            if hasattr(agent.noise, 'sigma'):
                agent.noise.sigma = noise_std
            
            actions = []
            for _ in range(50):
                if noise_std == 0.0:
                    action = agent.select_action(test_state, add_noise=False)
                else:
                    action = agent.select_action(test_state, add_noise=True)
                actions.append(action)
            
            actions_array = np.array(actions)
            action_diversity = np.std(actions_array, axis=0).mean()
            
            results['noise_effect'][f'noise_{noise_std}'] = {
                'action_diversity': action_diversity,
                'mean_action': np.mean(actions_array, axis=0).tolist(),
                'std_action': np.std(actions_array, axis=0).tolist()
            }
        
        # ì›ë˜ ë…¸ì´ì¦ˆ ê°•ë„ ë³µì›
        if hasattr(agent.noise, 'sigma'):
            agent.noise.sigma = original_noise_std
        
        # í†µê³„ ê³„ì‚°
        results['statistics'] = {
            'action_consistency_rate': np.mean(results['action_consistency']),
            'mean_action_variance': np.mean(results['action_variance']),
            'mean_noise_impact': results['noise_effect']['mean_noise_impact']
        }
        
        return results
    
    def compare_determinism(self, dqn_results: Dict, ddpg_results: Dict) -> Dict:
        """DQNê³¼ DDPGì˜ ê²°ì •ì„± ë¹„êµ"""
        print("ê²°ì •ì  ì •ì±… íŠ¹ì„± ë¹„êµ ë¶„ì„ ì¤‘...")
        
        comparison = {
            'determinism_scores': {},
            'consistency_comparison': {},
            'implementation_differences': {}
        }
        
        # 1. ê²°ì •ì„± ì ìˆ˜ ê³„ì‚° (0-1 ìŠ¤ì¼€ì¼)
        dqn_determinism = dqn_results['statistics']['action_consistency_rate']
        ddpg_determinism = ddpg_results['statistics']['action_consistency_rate']
        
        comparison['determinism_scores'] = {
            'dqn_score': dqn_determinism,
            'ddpg_score': ddpg_determinism,
            'difference': abs(dqn_determinism - ddpg_determinism)
        }
        
        # 2. ì¼ê´€ì„± ë©”ì»¤ë‹ˆì¦˜ ë¹„êµ
        comparison['consistency_comparison'] = {
            'dqn': {
                'mechanism': 'argmax over Q-values',
                'consistency_rate': dqn_determinism,
                'q_value_stability': dqn_results['statistics']['mean_q_consistency'],
                'decision_confidence': dqn_results['statistics']['mean_argmax_confidence']
            },
            'ddpg': {
                'mechanism': 'direct actor output',
                'consistency_rate': ddpg_determinism,
                'output_variance': ddpg_results['statistics']['mean_action_variance'],
                'noise_sensitivity': ddpg_results['statistics']['mean_noise_impact']
            }
        }
        
        # 3. êµ¬í˜„ ë°©ì‹ ì°¨ì´ì 
        comparison['implementation_differences'] = {
            'policy_representation': {
                'dqn': 'implicit (Q-values â†’ argmax)',
                'ddpg': 'explicit (actor network direct output)'
            },
            'action_space': {
                'dqn': 'discrete (finite set)',
                'ddpg': 'continuous (infinite set)'
            },
            'exploration_strategy': {
                'dqn': 'epsilon-greedy (probabilistic)',
                'ddpg': 'additive noise (deterministic + noise)'
            }
        }
        
        return comparison
    
    def generate_visualizations(self, dqn_results: Dict, ddpg_results: Dict, 
                              comparison: Dict, save_dir: str):
        """ê²°ì •ì  ì •ì±… ë¶„ì„ ì‹œê°í™” ìƒì„±"""
        print("ë¶„ì„ ê²°ê³¼ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        os.makedirs(save_dir, exist_ok=True)
        
        # 1. ê²°ì •ì„± ë¹„êµ ì°¨íŠ¸
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('DQN vs DDPG Deterministic Policy Analysis', fontsize=16, fontweight='bold')
        
        # DQN Q-ê°’ ì¼ê´€ì„±
        ax = axes[0, 0]
        ax.hist(dqn_results['q_value_consistency'], bins=20, alpha=0.7, 
                color='blue', edgecolor='black')
        ax.set_xlabel('Q-value Standard Deviation')
        ax.set_ylabel('Frequency')
        ax.set_title('DQN: Q-value Consistency')
        ax.grid(True, alpha=0.3)
        
        # DQN argmax ì‹ ë¢°ë„
        ax = axes[0, 1]
        ax.hist(dqn_results['argmax_confidence'], bins=20, alpha=0.7, 
                color='blue', edgecolor='black')
        ax.set_xlabel('Argmax Confidence')
        ax.set_ylabel('Frequency')
        ax.set_title('DQN: Argmax Confidence')
        ax.grid(True, alpha=0.3)
        
        # DQN Q-ê°’ ì°¨ì´
        ax = axes[0, 2]
        ax.hist(dqn_results['q_value_differences'], bins=20, alpha=0.7, 
                color='blue', edgecolor='black')
        ax.set_xlabel('Q-value Difference (Best - 2nd Best)')
        ax.set_ylabel('Frequency')
        ax.set_title('DQN: Q-value Differences')
        ax.grid(True, alpha=0.3)
        
        # DDPG ì•¡ì…˜ ë¶„ì‚°
        ax = axes[1, 0]
        ax.hist(ddpg_results['action_variance'], bins=20, alpha=0.7, 
                color='red', edgecolor='black')
        ax.set_xlabel('Action Variance')
        ax.set_ylabel('Frequency')
        ax.set_title('DDPG: Action Output Variance')
        ax.grid(True, alpha=0.3)
        
        # DDPG ë…¸ì´ì¦ˆ ì˜í–¥
        ax = axes[1, 1]
        ax.hist(ddpg_results['deterministic_vs_noisy'], bins=20, alpha=0.7, 
                color='red', edgecolor='black')
        ax.set_xlabel('Action Change due to Noise')
        ax.set_ylabel('Frequency')
        ax.set_title('DDPG: Action Change due to Noise')
        ax.grid(True, alpha=0.3)
        
        # ê²°ì •ì„± ì ìˆ˜ ë¹„êµ
        ax = axes[1, 2]
        algorithms = ['DQN', 'DDPG']
        scores = [comparison['determinism_scores']['dqn_score'], 
                 comparison['determinism_scores']['ddpg_score']]
        bars = ax.bar(algorithms, scores, color=['blue', 'red'], alpha=0.7)
        ax.set_ylabel('Determinism Score')
        ax.set_title('Deterministic Policy Consistency')
        ax.set_ylim(0, 1)
        ax.grid(True, alpha=0.3)
        
        # ê°’ í‘œì‹œ
        for bar, score in zip(bars, scores):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{score:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'deterministic_policy_analysis.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. ë…¸ì´ì¦ˆ íš¨ê³¼ ë¶„ì„ (DDPG ì „ìš©)
        if 'noise_effect' in ddpg_results:
            fig, ax = plt.subplots(1, 1, figsize=(10, 6))
            
            noise_levels = []
            diversities = []
            
            for key, value in ddpg_results['noise_effect'].items():
                if key.startswith('noise_') and key not in ['mean_noise_impact', 'noise_impact_std']:
                    try:
                        noise_level = float(key.split('_')[1])
                        diversity = value['action_diversity']
                        noise_levels.append(noise_level)
                        diversities.append(diversity)
                    except (ValueError, KeyError):
                        continue
            
            ax.plot(noise_levels, diversities, 'ro-', linewidth=2, markersize=8)
            ax.set_xlabel('Noise Standard Deviation')
            ax.set_ylabel('Action Diversity (Std)')
            ax.set_title('DDPG: Action Diversity vs Noise Level')
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(os.path.join(save_dir, 'ddpg_noise_effect.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close()
        
        print(f"ì‹œê°í™” ê²°ê³¼ ì €ì¥: {save_dir}")
    
    def run_full_analysis(self, dqn_model_path: str = None, ddpg_model_path: str = None):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("="*60)
        print("ê²°ì •ì  ì •ì±… ì‹¬ì¸µ ë¶„ì„ ì‹œì‘")
        print("="*60)
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        analysis_dir = os.path.join(self.results_dir, "deterministic_analysis")
        os.makedirs(analysis_dir, exist_ok=True)
        
        dqn_results = None
        ddpg_results = None
        
        # DQN ë¶„ì„
        if dqn_model_path and os.path.exists(dqn_model_path):
            print("\nDQN ëª¨ë¸ ë¡œë“œ ë° ë¶„ì„...")
            env = create_dqn_env("CartPole-v1")
            state_dim = env.observation_space.shape[0]
            action_dim = env.action_space.n
            
            agent = DQNAgent(state_dim, action_dim)
            agent.load(dqn_model_path)
            agent.q_network.eval()
            
            dqn_results = self.analyze_dqn_determinism(agent, env)
            env.close()
        else:
            print("\nDQN ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìƒ˜í”Œ ì—ì´ì „íŠ¸ë¡œ ë¶„ì„...")
            env = create_dqn_env("CartPole-v1")
            state_dim = env.observation_space.shape[0]
            action_dim = env.action_space.n
            
            agent = DQNAgent(state_dim, action_dim)
            dqn_results = self.analyze_dqn_determinism(agent, env)
            env.close()
        
        # DDPG ë¶„ì„
        if ddpg_model_path and os.path.exists(ddpg_model_path):
            print("\nDDPG ëª¨ë¸ ë¡œë“œ ë° ë¶„ì„...")
            env = create_ddpg_env("Pendulum-v1")
            state_dim = env.observation_space.shape[0]
            action_dim = env.action_space.shape[0]
            
            agent = DDPGAgent(state_dim, action_dim)
            agent.load(ddpg_model_path)
            agent.actor.eval()
            agent.critic.eval()
            
            ddpg_results = self.analyze_ddpg_determinism(agent, env)
            env.close()
        else:
            print("\nDDPG ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìƒ˜í”Œ ì—ì´ì „íŠ¸ë¡œ ë¶„ì„...")
            env = create_ddpg_env("Pendulum-v1")
            state_dim = env.observation_space.shape[0]
            action_dim = env.action_space.shape[0]
            
            agent = DDPGAgent(state_dim, action_dim)
            ddpg_results = self.analyze_ddpg_determinism(agent, env)
            env.close()
        
        # ë¹„êµ ë¶„ì„
        comparison = self.compare_determinism(dqn_results, ddpg_results)
        
        # ê²°ê³¼ ì €ì¥
        self.analysis_results = {
            'dqn_analysis': dqn_results,
            'ddpg_analysis': ddpg_results,
            'comparison': comparison
        }
        
        results_file = os.path.join(analysis_dir, "deterministic_policy_analysis.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            # numpy íƒ€ì…ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            json_serializable_results = self._convert_to_json_serializable(self.analysis_results)
            json.dump(json_serializable_results, f, indent=2, ensure_ascii=False)
        
        # ì‹œê°í™” ìƒì„±
        self.generate_visualizations(dqn_results, ddpg_results, comparison, analysis_dir)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        self.print_summary(comparison)
        
        print(f"\në¶„ì„ ì™„ë£Œ! ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {analysis_dir}")
        return self.analysis_results
    
    def print_summary(self, comparison: Dict):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ê²°ì •ì  ì •ì±… ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        print(f"\nğŸ“Š ê²°ì •ì„± ì ìˆ˜:")
        print(f"  â€¢ DQN:  {comparison['determinism_scores']['dqn_score']:.3f}")
        print(f"  â€¢ DDPG: {comparison['determinism_scores']['ddpg_score']:.3f}")
        print(f"  â€¢ ì°¨ì´:  {comparison['determinism_scores']['difference']:.3f}")
        
        print(f"\nğŸ” êµ¬í˜„ ë©”ì»¤ë‹ˆì¦˜:")
        dqn_info = comparison['consistency_comparison']['dqn']
        ddpg_info = comparison['consistency_comparison']['ddpg']
        
        print(f"  â€¢ DQN:")
        print(f"    - ë©”ì»¤ë‹ˆì¦˜: {dqn_info['mechanism']}")
        print(f"    - ì¼ê´€ì„±: {dqn_info['consistency_rate']:.3f}")
        print(f"    - Q-ê°’ ì•ˆì •ì„±: {dqn_info['q_value_stability']:.6f}")
        
        print(f"  â€¢ DDPG:")
        print(f"    - ë©”ì»¤ë‹ˆì¦˜: {ddpg_info['mechanism']}")
        print(f"    - ì¼ê´€ì„±: {ddpg_info['consistency_rate']:.3f}")
        print(f"    - ì¶œë ¥ ë¶„ì‚°: {ddpg_info['output_variance']:.6f}")
        
        print(f"\nğŸ’¡ í•µì‹¬ ì°¨ì´ì :")
        impl_diff = comparison['implementation_differences']
        print(f"  â€¢ ì •ì±… í‘œí˜„: DQN({impl_diff['policy_representation']['dqn']}) vs DDPG({impl_diff['policy_representation']['ddpg']})")
        print(f"  â€¢ í–‰ë™ ê³µê°„: DQN({impl_diff['action_space']['dqn']}) vs DDPG({impl_diff['action_space']['ddpg']})")
        print(f"  â€¢ íƒí—˜ ì „ëµ: DQN({impl_diff['exploration_strategy']['dqn']}) vs DDPG({impl_diff['exploration_strategy']['ddpg']})")
    
    def _convert_to_json_serializable(self, obj):
        """numpy íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
        if isinstance(obj, dict):
            return {key: self._convert_to_json_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_json_serializable(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, (np.int32, np.int64)):
            return int(obj)
        elif isinstance(obj, np.bool_):
            return bool(obj)
        else:
            return obj


def main():
    parser = argparse.ArgumentParser(description='ê²°ì •ì  ì •ì±… ì‹¬ì¸µ ë¶„ì„')
    parser.add_argument('--dqn-model', type=str, 
                       help='DQN ëª¨ë¸ ê²½ë¡œ (ì„ íƒì‚¬í•­)')
    parser.add_argument('--ddpg-model', type=str, 
                       help='DDPG ëª¨ë¸ ê²½ë¡œ (ì„ íƒì‚¬í•­)')
    parser.add_argument('--results-dir', type=str, default='results',
                       help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--seed', type=int, default=42,
                       help='ëœë¤ ì‹œë“œ')
    
    args = parser.parse_args()
    
    # ì‹œë“œ ì„¤ì •
    set_seed(args.seed)
    
    # ë¶„ì„ ì‹¤í–‰
    analyzer = DeterministicPolicyAnalyzer(args.results_dir)
    analyzer.run_full_analysis(args.dqn_model, args.ddpg_model)


if __name__ == "__main__":
    main()