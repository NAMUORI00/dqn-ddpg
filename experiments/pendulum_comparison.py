#!/usr/bin/env python3
"""
Pendulum-v1 í™˜ê²½ì—ì„œ DDPG vs DQN ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜

DDPGê°€ ìš°ìˆ˜í•œ í™˜ê²½ì—ì„œì˜ ë¹„êµë¥¼ í†µí•´ ì•Œê³ ë¦¬ì¦˜ ì í•©ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from typing import Dict, List, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

from src.agents.ddpg_agent import DDPGAgent
from src.agents.dqn_agent import DQNAgent  
from src.environments.wrappers import create_ddpg_env, create_dqn_env
from src.core.utils import set_seed
import gymnasium as gym


class PendulumDQNWrapper(gym.Wrapper):
    """Pendulum í™˜ê²½ì„ DQNìš©ìœ¼ë¡œ ì´ì‚°í™”í•˜ëŠ” ë˜í¼"""
    
    def __init__(self, env: gym.Env, num_actions: int = 21):
        super().__init__(env)
        self.num_actions = num_actions
        
        # ì—°ì† í–‰ë™ [-2, 2]ë¥¼ ì´ì‚° í–‰ë™ìœ¼ë¡œ ë³€í™˜
        self.action_space = gym.spaces.Discrete(num_actions)
        self.action_mapping = np.linspace(-2.0, 2.0, num_actions)
        
    def step(self, action: int) -> Tuple:
        # ì´ì‚° í–‰ë™ì„ ì—°ì† í–‰ë™ìœ¼ë¡œ ë³€í™˜
        continuous_action = np.array([self.action_mapping[action]], dtype=np.float32)
        return self.env.step(continuous_action)


def create_pendulum_dqn_env() -> gym.Env:
    """DQNìš© Pendulum í™˜ê²½ ìƒì„±"""
    env = gym.make("Pendulum-v1")
    env = PendulumDQNWrapper(env, num_actions=21)
    return env


def train_agent(agent, env, max_episodes: int = 200, eval_frequency: int = 25) -> Dict:
    """ì—ì´ì „íŠ¸ í•™ìŠµ ë° ì„±ëŠ¥ í‰ê°€"""
    scores = []
    eval_scores = []
    eval_episodes = []
    
    print(f"Training {agent.__class__.__name__} for {max_episodes} episodes...")
    
    for episode in range(max_episodes):
        state, _ = env.reset()
        total_reward = 0
        step_count = 0
        max_steps = 200  # Pendulum ê¸°ë³¸ step limit
        
        while step_count < max_steps:
            action = agent.select_action(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            
            agent.store_transition(state, action, reward, next_state, terminated or truncated)
            agent.update()
            
            state = next_state
            total_reward += reward
            step_count += 1
            
            if terminated or truncated:
                break
        
        scores.append(total_reward)
        
        # ì£¼ê¸°ì  í‰ê°€
        if (episode + 1) % eval_frequency == 0:
            eval_score = evaluate_agent(agent, env, episodes=10)
            eval_scores.append(eval_score)
            eval_episodes.append(episode + 1)
            print(f"Episode {episode + 1}: Score = {total_reward:.2f}, Eval Score = {eval_score:.2f}")
    
    return {
        'scores': scores,
        'eval_scores': eval_scores,
        'eval_episodes': eval_episodes,
        'final_score': np.mean(scores[-100:]) if len(scores) >= 100 else np.mean(scores),
        'best_score': max(scores),
        'final_eval_score': eval_scores[-1] if eval_scores else 0
    }


def evaluate_agent(agent, env, episodes: int = 10) -> float:
    """ì—ì´ì „íŠ¸ ì„±ëŠ¥ í‰ê°€"""
    total_rewards = []
    
    for _ in range(episodes):
        state, _ = env.reset()
        total_reward = 0
        step_count = 0
        max_steps = 200
        
        while step_count < max_steps:
            action = agent.select_action(state, add_noise=False)  # íƒí—˜ ì—†ì´ í‰ê°€
            next_state, reward, terminated, truncated, _ = env.step(action)
            
            state = next_state
            total_reward += reward
            step_count += 1
            
            if terminated or truncated:
                break
        
        total_rewards.append(total_reward)
    
    return np.mean(total_rewards)


def run_pendulum_comparison():
    """Pendulum í™˜ê²½ì—ì„œ DDPG vs DQN ë¹„êµ ì‹¤í—˜ ì‹¤í–‰"""
    print("ğŸ¯ Pendulum-v1 í™˜ê²½ì—ì„œ DDPG vs DQN ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
    print("=" * 60)
    
    # ì‹œë“œ ì„¤ì •
    seed = 42
    set_seed(seed)
    
    # í™˜ê²½ ì„¤ì •
    ddpg_env = create_ddpg_env("Pendulum-v1")
    dqn_env = create_pendulum_dqn_env()
    
    # ì—ì´ì „íŠ¸ ì„¤ì •
    state_dim = 3  # cos(Î¸), sin(Î¸), Î¸_dot
    action_dim = 1  # í† í¬
    
    ddpg_agent = DDPGAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        actor_lr=0.0001,
        critic_lr=0.001,
        gamma=0.99,
        tau=0.005,
        buffer_size=50000,
        batch_size=64,
        noise_sigma=0.1
    )
    
    dqn_agent = DQNAgent(
        state_dim=state_dim,
        action_dim=21,  # ì´ì‚°í™”ëœ í–‰ë™ ìˆ˜
        learning_rate=0.001,
        epsilon=1.0,
        epsilon_min=0.01,
        epsilon_decay=0.995,
        gamma=0.99,
        buffer_size=50000,
        batch_size=64,
        target_update_freq=100
    )
    
    # í•™ìŠµ ì‹¤í–‰ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¶•ì†Œ)
    max_episodes = 200
    
    print("ğŸ¤– DDPG í•™ìŠµ ì‹œì‘...")
    ddpg_results = train_agent(ddpg_agent, ddpg_env, max_episodes)
    
    print("\nğŸ¤– DQN í•™ìŠµ ì‹œì‘...")
    dqn_results = train_agent(dqn_agent, dqn_env, max_episodes)
    
    # ê²°ê³¼ ì •ë¦¬
    results = {
        'experiment_config': {
            'environment': 'Pendulum-v1',
            'max_episodes': max_episodes,
            'seed': seed,
            'ddpg_config': {
                'actor_lr': 0.0001,
                'critic_lr': 0.001,
                'gamma': 0.99,
                'tau': 0.005,
                'noise_sigma': 0.1,
                'buffer_size': 50000,
                'batch_size': 64
            },
            'dqn_config': {
                'learning_rate': 0.001,
                'epsilon_decay': 0.995,
                'gamma': 0.99,
                'buffer_size': 50000,
                'batch_size': 64,
                'num_actions': 21
            }
        },
        'training_results': {
            'ddpg': ddpg_results,
            'dqn': dqn_results
        },
        'comparison': {
            'ddpg_final': ddpg_results['final_score'],
            'dqn_final': dqn_results['final_score'],
            'performance_ratio': abs(ddpg_results['final_score'] / dqn_results['final_score']) if dqn_results['final_score'] != 0 else float('inf'),
            'ddpg_advantage': ddpg_results['final_score'] > dqn_results['final_score']
        },
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    print(f"DDPG ìµœì¢… ì„±ëŠ¥: {ddpg_results['final_score']:.2f}")
    print(f"DQN ìµœì¢… ì„±ëŠ¥:  {dqn_results['final_score']:.2f}")
    print(f"ì„±ëŠ¥ ë¹„ìœ¨:      {results['comparison']['performance_ratio']:.2f}x")
    print(f"DDPG ìš°ìœ„:      {'âœ…' if results['comparison']['ddpg_advantage'] else 'âŒ'}")
    
    # ê²°ê³¼ ì €ì¥
    os.makedirs('results/pendulum_comparison', exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    result_file = f'results/pendulum_comparison/pendulum_results_{timestamp}.json'
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {result_file}")
    
    # ì‹œê°í™” ìƒì„±
    create_pendulum_visualizations(results, timestamp)
    
    return results


def create_pendulum_visualizations(results: Dict, timestamp: str):
    """Pendulum ì‹¤í—˜ ê²°ê³¼ ì‹œê°í™” ìƒì„±"""
    ddpg_scores = results['training_results']['ddpg']['scores']
    dqn_scores = results['training_results']['dqn']['scores']
    
    # 1. í•™ìŠµ ê³¡ì„  ë¹„êµ
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    episodes = range(1, len(ddpg_scores) + 1)
    plt.plot(episodes, ddpg_scores, label='DDPG', color='blue', alpha=0.7)
    plt.plot(episodes, dqn_scores, label='DQN', color='red', alpha=0.7)
    
    # ì´ë™ í‰ê·  ì¶”ê°€
    window = 50
    if len(ddpg_scores) >= window:
        ddpg_ma = np.convolve(ddpg_scores, np.ones(window)/window, mode='valid')
        dqn_ma = np.convolve(dqn_scores, np.ones(window)/window, mode='valid')
        ma_episodes = range(window, len(ddpg_scores) + 1)
        plt.plot(ma_episodes, ddpg_ma, label='DDPG (MA50)', color='darkblue', linewidth=2)
        plt.plot(ma_episodes, dqn_ma, label='DQN (MA50)', color='darkred', linewidth=2)
    
    plt.title('Pendulum-v1: Learning Curves Comparison')
    plt.xlabel('Episodes')
    plt.ylabel('Reward')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 2. ìµœì¢… ì„±ëŠ¥ ë¹„êµ
    plt.subplot(2, 2, 2)
    algorithms = ['DDPG', 'DQN']
    final_scores = [
        results['comparison']['ddpg_final'],
        results['comparison']['dqn_final']
    ]
    colors = ['blue', 'red']
    
    bars = plt.bar(algorithms, final_scores, color=colors, alpha=0.7)
    plt.title('Final Performance Comparison')
    plt.ylabel('Average Reward (Last 100 episodes)')
    
    # ê°’ í‘œì‹œ
    for bar, score in zip(bars, final_scores):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,
                f'{score:.1f}', ha='center', va='bottom', fontweight='bold')
    
    plt.grid(True, alpha=0.3, axis='y')
    
    # 3. ì„±ëŠ¥ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
    plt.subplot(2, 2, 3)
    plt.hist(ddpg_scores[-100:], bins=20, alpha=0.5, label='DDPG', color='blue', density=True)
    plt.hist(dqn_scores[-100:], bins=20, alpha=0.5, label='DQN', color='red', density=True)
    plt.title('Performance Distribution (Last 100 episodes)')
    plt.xlabel('Reward')
    plt.ylabel('Density')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 4. í†µê³„ ì •ë³´
    plt.subplot(2, 2, 4)
    plt.axis('off')
    
    stats_text = f"""
Pendulum-v1 Experiment Results

DDPG Performance:
â€¢ Final Score: {results['comparison']['ddpg_final']:.2f}
â€¢ Best Score: {results['training_results']['ddpg']['best_score']:.2f}
â€¢ Std Dev: {np.std(ddpg_scores[-100:]):.2f}

DQN Performance:
â€¢ Final Score: {results['comparison']['dqn_final']:.2f}
â€¢ Best Score: {results['training_results']['dqn']['best_score']:.2f}
â€¢ Std Dev: {np.std(dqn_scores[-100:]):.2f}

Comparison:
â€¢ Performance Ratio: {results['comparison']['performance_ratio']:.2f}x
â€¢ DDPG Advantage: {'âœ… Yes' if results['comparison']['ddpg_advantage'] else 'âŒ No'}

Environment: Pendulum-v1 (Continuous Control)
Episodes: {results['experiment_config']['max_episodes']}
Timestamp: {results['timestamp']}
"""
    
    plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes,
             verticalalignment='top', fontfamily='monospace', fontsize=9)
    
    plt.tight_layout()
    
    # ì €ì¥
    viz_file = f'results/pendulum_comparison/pendulum_visualization_{timestamp}.png'
    plt.savefig(viz_file, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š ì‹œê°í™” ì €ì¥: {viz_file}")
    
    plt.show()


if __name__ == "__main__":
    results = run_pendulum_comparison()